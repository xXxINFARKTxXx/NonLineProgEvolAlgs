{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Список вопросов для экзамена по оптимизации 2021-2022 учебный год"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Одномерная оптимизация: Необходимые и достаточные условия\n",
    "\n",
    "Одномерная оптимизация означает поиск оптимального значения для функции одной переменной. Ниже представлены необходимые и достаточные условия для точки экстремума в одномерной оптимизации:\n",
    "\n",
    "### Необходимые условия:\n",
    "\n",
    "1. **Стационарная точка:** Если функция $ f(x) $ имеет локальный экстремум в точке $ x = x^* $, то производная функции в этой точке должна быть равна нулю: \n",
    "   $$ f'(x^*) = 0 $$\n",
    "\n",
    "### Достаточные условия:\n",
    "\n",
    "1. **Вторая производная:** Проверка знака второй производной в точке $ x^* $ позволяет определить характер экстремума:\n",
    "   - Если $ f''(x^*) > 0 $, то это точка локального минимума.\n",
    "   - Если $ f''(x^*) < 0 $, то это точка локального максимума.\n",
    "\n",
    "2. **Тест первой производной (знак изменения производной):**\n",
    "   - Если $ f'(x) $ меняет знак с положительного на отрицательный при переходе через точку $ x^* $, то $ x^* $ - локальный максимум.\n",
    "   - Если $ f'(x) $ меняет знак с отрицательного на положительный при переходе через точку $ x^* $, то $ x^* $ - локальный минимум.\n",
    "\n",
    "Эти условия позволяют анализировать свойства точек экстремума в одномерной оптимизации.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Многомерная оптимизация без ограничений\n",
    "### Разложение в ряд Тейлора\n",
    "\n",
    "Развитие функции многих переменных в ряд Тейлора\n",
    "\n",
    "Рассматриваем функцию многих переменных $y(x) = y(x_1, x_2, x_3, . . . , x_n )$, которая представляет собой критерий оптимальности, чей минимум/максимум мы пытаемся определить. Здесь $x$ - вектор переменных состояния, обозначенный и определенный как $x = [x_1, x_2, x_3, . . . , x_n ]^T$.\n",
    "$$\n",
    "y(x) = y(x1,x2,x3,...,x_n)\\tag{1}  \n",
    "$$\n",
    "Предполагая, что функция (1) определена и непрерывна вместе со своими $j + 1$ производными в окрестности точки $x^*$, тогда обычно разложение в ряд Тейлора в его общей форме записывается как:\n",
    "\n",
    "$$\n",
    "    y(x) = \n",
    "        y(x^*) + \n",
    "        \\frac{\\Delta y}{1!} + \n",
    "        \\frac{\\Delta^2 y}{2!} + \n",
    "        . . . +\n",
    "        \\frac{\\Delta^j y}{j!} + R_{j+1} \n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "где $R_{j+1}$ - остаток разложения в ряд, а остальные члены, вместе с аналогичным обобщением на более высокие производные, представлены следующим образом:\n",
    "\n",
    "$$\n",
    "    \\Delta y = \n",
    "        (x_1 - x^*_1) \\frac{\\partial y}{\\partial x_1}\\Big|_{x^*} + \n",
    "        (x_2 - x^*_2) \\frac{\\partial y}{\\partial x_2}\\Big|_{x^*} + \n",
    "        \\ldots + \n",
    "        (x_n - x^*_n) \\frac{\\partial y}{\\partial x_n}\\Big|_{x^*}\n",
    "\n",
    "\\newline\n",
    "\n",
    "    \\Delta^2 y = \n",
    "        (x_1 - x^*_1)^2 \\frac{\\partial^2 y}{\\partial x_1^2}\\Big|_{x^*} + \n",
    "        2(x_1 - x^*_1) (x_2 - x^*_2) \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}\\Big|_{x^*} + \n",
    "        (x_2 - x^*_2)^2\\frac{\\partial^2 y}{\\partial x_2^2}\\Big|_{x^*} + \n",
    "        \\ldots + \n",
    "        (x_n - x^*_n)^2\\frac{\\partial^2 y}{\\partial x_n^2}\\Big|_{x^*}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "(и так далее для более высоких порядков).\n",
    "\n",
    "В общем виде:\n",
    "\n",
    "$$\n",
    "    \\Delta^k y = \n",
    "        \\sum_{p_1, p_2, \\ldots, p_n} \n",
    "            \\frac{k!}{p_1! p_2! \\ldots p_n!} \n",
    "            (x_1 - x^*_1)^{p_1} (x_2 - x^*_2)^{p_2} \\ldots (x_n - x^*_n)^{p_n} \n",
    "            \\frac{\\partial^k y}{\\partial x_1^{p_1} \\partial x_2^{p_2} \\ldots \\partial x_n^{p_n}}\\Big|_{x^*}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "Здесь $p_1, p_2, ..., p_n$ - неотрицательные целые числа, и суммирование происходит по всем возможным наборам $p_1, p_2, ..., p_n$ таким, что $p_1 + p_2 + ... + p_n = k$.\n",
    "\n",
    "Тогда для функции двух переменных \n",
    "$$ \n",
    "    y(\\hat{x}) = y(x1, x2) \n",
    "\\tag{5}\n",
    "$$\n",
    ":\n",
    "$$\n",
    "    \\Delta y = \n",
    "        (x_1 - x^*_1)\\frac{\\partial y}{\\partial x_1}\\Big|_{x^*} +         \n",
    "        (x_2 - x^*_2)\\frac{\\partial y}{\\partial x_2}\\Big|_{x^*}\n",
    "\n",
    "\\newline\n",
    "\n",
    "    \\Delta^2 y = \n",
    "        (x_1 - x^*_1)^2\\frac{\\partial^2 y}{\\partial x_1^2}\\Big|_{x^*} + \n",
    "        2(x_1 - x^*_1)(x_2 - x^*_2) \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}\\Big|_{x^*} + \n",
    "        (x_2 - x^*_2)^2\\frac{\\partial^2 y}{\\partial x_2^2}\\Big|_{x^*}\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "\n",
    "Применяя рассуждения из одномерной оптимизации, мы можем ввести замену $x_i - x^*_i = h_i$ и упростить выражения (6), записав их в более компактной форме, более удобной для дальнейшего анализа:\n",
    "\n",
    "$$\n",
    "    \\Delta y = \n",
    "        h_1 \\frac{\\partial y}{\\partial x_1}\\Big|_{x^*} +\n",
    "        h_2 \\frac{\\partial y}{\\partial x_2}\\Big|_{x^*}\n",
    "\n",
    "\\newline\n",
    "\n",
    "    \\Delta^2 y = \n",
    "        h_1^2 \\frac{\\partial^2 y}{\\partial x_1^2}\\Big|_{x^*} + \n",
    "        2h_1 h_2 \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}\\Big|_{x^*} + \n",
    "        h_2^2 \\frac{\\partial^2 y}{\\partial x_2^2}\\Big|_{x^*}\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "### Необходимые и достаточные условия экстремума функции двух переменных\n",
    "\n",
    "Необходимые и достаточные условия для экстремума функции двух переменных напоминают случай функции одной переменной. Напоминаем выражение для разницы функции одной переменной:\n",
    "\n",
    "$$y(x^* + h) - y(x^*) = h y'(x^*) + \\frac{h^2}{2} y''(x^*)$$\n",
    "\n",
    "Здесь, из-за колебаний знака $h$, мы вводим необходимое условие того, что первая производная в точке $x^*$ равна нулю, то есть $y'(x^*) = 0$. Аналогичное рассуждение применяется и для функции двух переменных.\n",
    "\n",
    "В случае функции нескольких переменных, сводится к условию, что все частные производные по переменным $x_i$ равны нулю. То есть:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial y}{\\partial x_i}\\Bigg|_{x^*} = 0 \\text{ для всех i = 1, 2}  \\tag{8}\n",
    "$$.\n",
    "\n",
    "Все кандидаты на (локальный) минимум/максимум находятся среди стационарных точек. Это утверждение можно сформулировать следующим интуитивным образом.\n",
    "\n",
    "> **Утверждение 1. (Необходимые условия экстремума функции двух переменных):** В случае, если существуют частные производные первого порядка функции $y$ по всем переменным, то точка экстремума функции $y$ в точке $x^*$ удовлетворяет системе уравнений $$ \\frac{\\partial y}{\\partial x_i}\\Bigg|_{x^*} = 0 \\quad \\text{ для } i = 1, 2 $$ или условию $$ \\Delta y = 0 $$.\n",
    "\n",
    "Если точка $ (x^*_1, x^*_2) $ удовлетворяет условиям стационарности (необходимые условия экстремума), то она считается стационарной точкой. Тогда, необходимые условия для экстремума могут быть выражены следующим образом:\n",
    "\n",
    "$$ y(x) - y(x^*) = \\frac{\\Delta^2 y}{2!} \\tag{9}$$\n",
    "\n",
    "где $ \\Delta^2 y $ представляет собой изменение производной функции второго порядка, а $ y(x^*) $ - значение функции в стационарной точке $ (x^*_1, x^*_2) $.\n",
    "\n",
    "Далее, из предыдущего выражения ясно видно, что стационарная точка $ (x^*_1, x^*_2) $ имеет максимальное значение, если правая часть уравнения меньше нуля, то есть минимальное значение достигается при $ \\Delta^2 y < 0 $. Предположим, что нас интересует, когда функция (5) достигает максимального значения, то есть когда выражение $ \\Delta^2 y < 0 $:\n",
    "\n",
    "$$\n",
    "    \\Delta^2 y = \n",
    "        h^2_{1} \\frac{\\partial^2 y}{\\partial x_{1}^2} \\Bigr|_{\\ast} + \n",
    "        2h_{1}h_{2} \\frac{\\partial^2 y}{\\partial x_{1} \\partial x_{2}} \\Bigr|_{\\ast} + \n",
    "        h^2_{2} \\frac{\\partial^2 y}{\\partial x_{2}^2} \\Bigr|_{\\ast} < 0 \n",
    "    \\text{ при нетривиальных } h_1 \\text{ и } h_2 \n",
    "\\tag{11}\n",
    "$$\n",
    "\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y > 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ положительно определена.\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y \\leq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ отрицательно полуопределена.\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y \\geq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ положительно полуопределена.\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y \\neq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ неопределена.\n",
    "\n",
    "\n",
    "Если условие (11) выполняется, мы утверждаем, что функция $ \\Delta^2 y $ отрицательно определена. Обычно в литературе понятие определенности связано с концепциями минимума и максимума, а процесс выведения достаточных условий называется исследованием определенности. Как вы увидите, это не тривиальная задача и, вероятно, является наиболее сложной в этой части курса.\n",
    "\n",
    "У нашем конкретном случае, наша цель - вывести достаточные условия, при которых справедливы соотношения (10) и (11). Это сводится к тому, чтобы представить эти уравнения в компактной форме, в которой все члены $h_1$ и $h_2$ будут под знаком квадрата, что гарантирует инвариантность условий относительно знака параметров $h_i$.\n",
    "\n",
    "В процессе вывода, который следует, мы следуем логике, которая приведет нас к условиям, гарантирующим отрицательную (положительную) определенность. Другие граничные случаи мы рассмотрим через примеры, и они не являются предметом данного вопроса. Выражение (10) мы можем разложить следующим образом:\n",
    "\n",
    "$$ \n",
    "    \\Delta^2 y = \n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "    \\left[\n",
    "        h^2_{1} +\n",
    "        2h_1 h_2 \\frac{\n",
    "            \\left( \\frac{\\partial^2 y}{\\partial x_{1}\\partial x_{2}} \\right)^*\n",
    "        }{\n",
    "            \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "        } +\n",
    "        h^2_2 \\frac{\n",
    "            \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^*\n",
    "        }{\n",
    "            \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "        }\n",
    "    \\right] < 0\n",
    "\\tag{12}\n",
    "$$\n",
    "\n",
    "\n",
    "Первые два члена в скобках можно рассматривать как часть бинома, а остальные члены мы сгруппируем отдельно. В выражении (13) мы достигли того, что все члены $h_i$ теперь под знаком квадрата, и их вклад в характер экстремума теперь не вызывает сомнений. Таким образом, на определенность теперь влияют только члены, для которых мы предполагаем, что они меньше 0.\n",
    "\n",
    "$$ \n",
    "    \\Delta^2 y = \n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "        \\left[\n",
    "            h^2_{1} +\n",
    "            2h_1 h_2 \\frac{\n",
    "                \\left( \\frac{\\partial^2 y}{\\partial x_{1}\\partial x_{2}} \\right)^*\n",
    "            }{\n",
    "                \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "            }\n",
    "        \\right]^{2} +\n",
    "        h^2_2 \\left[\n",
    "            \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^* - \n",
    "            \\frac{\n",
    "                \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^{*2}\n",
    "            }{\n",
    "                \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "            }\n",
    "        \\right] < 0\n",
    "\\tag{13}\n",
    "$$\n",
    "\n",
    "В выражении (13) мы добились того, что все члены $h_i$ теперь под знаком квадрата, и их вклад в характер экстремума теперь не вызывает сомнений, то есть на определенность теперь влияют только члены, для которых мы предполагаем, что они меньше 0.\n",
    "\n",
    "$$\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^* < 0 \n",
    "\\text{  и  }\n",
    "    \\left[\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^* - \n",
    "        \\frac{\n",
    "            \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^{*2}\n",
    "        }{\n",
    "            \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "        }\n",
    "    \\right] < 0\n",
    "\\tag{14}\n",
    "$$\n",
    "\n",
    "Выражение в средней скобке может быть записано в виде рационального выражения, а именно, как\n",
    "\n",
    "$$\n",
    "    \\frac{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^*\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "        -\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}\\partial x_{1} \\partial x_{2}} \\right)^{*2}\n",
    "    }{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "    } < 0\n",
    "\\tag{15}\n",
    "$$\n",
    "Перевод на русский:\n",
    "\n",
    "Первое условие из выражения (14),\n",
    "$\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^* < 0\n",
    "$\n",
    "однозначно указывает на то, что знаменатель выражения (15) должен быть меньше нуля, а числитель должен быть больше нуля, следовательно, условия отрицательной определённости, наконец, становятся:\n",
    "$$\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^* < 0 \n",
    "\\tag{16}\n",
    "$$\n",
    "и\n",
    "$$\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^*\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^* -\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}\\partial x_{1} \\partial x_{2}} \\right)^{*2} > 0\n",
    "\\tag{17}\n",
    "$$\n",
    "\n",
    "Выражения (16) и (17) представляют собой достаточные условия того, что функция двух переменных имеет максимум, то есть что функция (10) отрицательно определена.\n",
    "\n",
    "Читателям предоставляется возможность самостоятельно вывести условия положительной определённости, которые будут получены без значительных трудностей в следующей форме:\n",
    "$$\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^* > 0 \n",
    "\\tag{18}\n",
    "$$\n",
    "$$\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^*\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^* -\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{1}\\partial x_{1} \\partial x_{2}} \\right)^{*2} > 0\n",
    "\\tag{19}\n",
    "$$\n",
    "\n",
    "Перевод на русский:\n",
    "\n",
    "Тем не менее, важно подчеркнуть, что стационарная точка может быть как максимумом, так и минимумом, когда функция $ \\Delta^2 y $ является отрицательно полуопределённой или положительно полуопределённой соответственно. О этих случаях мы упомянем в последующих примерах, а читателей направляем на рекомендованную литературу для получения более подробной информации.\n",
    "\n",
    "Условия определённости (16)-(19) можно более компактно записать с использованием критерия Лагранжа следующим образом:\n",
    "\n",
    "> Утверждение 2. (Достаточные условия экстремума функции двух переменных)\n",
    "> В случае, если существуют частные производные второго порядка функции $ y $ по всем переменным в окрестности точки $ x^* $, и если выполняется условие $\\frac{\\partial y}{\\partial x_i}\\big|_{x^*} = 0$, то точка экстремума функции $ y $ в точке $ x^* $ удовлетворяет следующим условиям: $ \\newline $\n",
    "> Для строгого локального минимума: $D_i > 0$ для $i = 1, 2$. $ \\newline $\n",
    "> Для строгого локального максимума: $D_1 < 0$ и $D_2 > 0$, где $ \\newline $\n",
    "> $$ D_1 = \\left|\\frac{\\partial^2 y}{\\partial x_1^2}\\right|_{x=x^*}, \\quad D_2 = \\left|\\begin{matrix} \\newline \\frac{\\partial^2 y}{\\partial x_1^2} & \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2} \\newline \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2} & \\frac{\\partial^2 y}{\\partial x_2^2} \\newline \\end{matrix}\\right|_{x=x^*} $$\n",
    "\n",
    "\n",
    "Введенные достаточные условия представляют собой очень строгие требования, которые функция должна удовлетворять в отношении экстремума.\n",
    "\n",
    "### Необходимые и достаточные условия экстремума функции многих переменных\n",
    "\n",
    "\"Потребные и достаточные условия функции многих переменных (*В нашем случае двух переменных*) мы не будем отдельно выводить и доказывать, а логически обобщим изучение из предыдущего параграфа. Рассматриваем функцию, которая в общем случае зависит от n переменных:\n",
    "$$ y(x) = y(x_1, x_2, x_3, \\ldots, x_n) . \\tag{20} $$\n",
    "Как мы показали, разложение в ряд функции многих переменных сводится к выражению (2), при этом наиболее значимыми являются первые два члена с правой стороны равенства, то есть\n",
    "$$ y(x) - y(x^*) = \\Delta y + \\frac{\\Delta^2 y}{2} . \\tag{21} $$\n",
    "Необходимые условия экстремума получаются обобщением Утверждения 1.2\n",
    "> Утверждение 3. (Необходимые условия экстремума функции многих переменных)\n",
    "> В случае наличия частных производных первого порядка функции $y$ по всем переменным, точка экстремума функции $y$ в точке $x^*$ удовлетворяет системе уравнений\n",
    "> $$ \\frac{\\partial y}{\\partial x_i}\\Big|_{x^*} = 0 \\text{ для } i = 1, \\ldots , n $$\n",
    "> или условию\n",
    "> $$ \\Delta y = 0 $$\n",
    "\n",
    "Чтобы описать сложные связи из условий (16)-(19) в случае функции двух переменных, мы ввели матричную форму вторых производных, которая для функции (20) может быть записана следующим образом:\n",
    "$$\n",
    "    H = \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial^2 y}{\\partial x_1^2} & \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2} & \\frac{\\partial^2 y}{\\partial x_1 \\partial x_3} & \\ldots & \\frac{\\partial^2 y}{\\partial x_1 \\partial x_n} \\\\\n",
    "        \\frac{\\partial^2 y}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 y}{\\partial x_2^2} & \\frac{\\partial^2 y}{\\partial x_2 \\partial x_3} & \\ldots & \\frac{\\partial^2 y}{\\partial x_2 \\partial x_n} \\\\\n",
    "        \\frac{\\partial^2 y}{\\partial x_3 \\partial x_1} & \\frac{\\partial^2 y}{\\partial x_3 \\partial x_2} & \\frac{\\partial^2 y}{\\partial x_3^2} & \\ldots & \\frac{\\partial^2 y}{\\partial x_3 \\partial x_n} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\partial^2 y}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 y}{\\partial x_n \\partial x_2} & \\frac{\\partial^2 y}{\\partial x_n \\partial x_3} & \\ldots & \\frac{\\partial^2 y}{\\partial x_n^2}\n",
    "    \\end{bmatrix}\n",
    "\\tag{22}\n",
    "$$\n",
    "\n",
    "Эта матрица (22) называется Гессианом, и она играет важную роль в наших последующих рассмотрениях.\n",
    "\n",
    "Как мы видели на примере функции двух переменных, определение закономерностей, когда квадратичная форма (7) положительно/отрицательно определена, не является простым и дополнительно усложняется в случае, когда функция зависит от более чем двух переменных (3). Для проверки определённости, как правило, используется Теорема Сильвестра.\n",
    "\n",
    "> Утверждение 4. (Сильвестрова теорема, условия определённости) Симметричная квадратная матрица $A = [a_{ij}]$ размерности $n$ является положительно определённой тогда и только тогда, когда все главные миноры (миноры вокруг главной диагонали) матрицы $A$ положительны, то есть $D_i > 0$ для $i = 1, 2, \\ldots, n$, и отрицательно определённой тогда и только тогда, когда главные миноры (миноры вокруг главной диагонали) матрицы $A$ чередуют знак следующим образом: $ \\newline $\n",
    "> $\\newline \\newline D_{1,3,5,7,...} < 0, \\quad D_{2,4,6,8,...} > 0 \\newline \\newline $\n",
    "> где $ \\newline $\n",
    "> $\\newline D_1 = \\left|a_{11}\\right|, \\quad D_2 = \\left|\\begin{array}{cc} a_{11} & a_{12} \\\\ a_{12} & a_{22} \\end{array}\\right|, \\quad \\ldots, \\quad D_n = \\left|\\begin{array}{cccc} a_{11} & a_{12} & a_{13} & \\ldots & a_{1n} \\\\ a_{21} & a_{22} & a_{23} & \\ldots & a_{2n} \\\\ a_{31} & a_{32} & a_{33} & \\ldots & a_{3n} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n1} & a_{n2} & a_{n3} & \\ldots & a_{nn} \\end{array}\\right| \\newline $\n",
    "\n",
    "Наконец, легко заметить, что квадратная форма функции нескольких переменных, для которой мы исследуем определённость (3), фактически представлена гессианом (22). Исследование определённости сводится к анализу главных миноров матрицы Гессе в соответствии с Тверджением 4.\n",
    "\n",
    "> Утверждение 5. (Достаточные условия экстремума функции нескольких переменных) В случае наличия частных производных второго порядка функции $y$ по всем переменным в окрестности точки $x^*$ и при условии $\\Delta^2 y\\big|_{x^*} = 0$, точка экстремума функции $y$ в точке $x^*$ удовлетворяет следующим условиям:\n",
    "> - Для строгого локального минимума, $D_i > 0$ для $i = 1, 2, \\ldots, n$.\n",
    "> - Для строгого локального максимума, $D_{1,3,5,7,...} < 0$, $D_{2,4,6,8,...} > 0$, где\n",
    "> $$\n",
    "> D_1 = \\left|\\frac{\\partial^2 y}{\\partial x_1^2}\\right|_{x=x^*}, \\quad D_2 = \\left|\\begin{array}{cc} \\frac{\\partial^2 y}{\\partial x_1^2} & \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2} & \\frac{\\partial^2 y}{\\partial x_2^2} \\end{array}\\right|_{x=x^*}, \\ldots, \\quad D_n = \\left|\\frac{\\partial^2 y}{\\partial x_n^2}\\right|_{x=x^*},\n",
    "> $$\n",
    "> и матрица $H$ имеет форму, представленную в выражении (22).\n",
    "\n",
    "\n",
    "Хорошо, зададим следующий вопрос: если условие для строгого минимума/максимума является положительной/отрицательной определенностью функции, будет ли под теми же условиями полуопределенность гарантировать минимум/максимум (без строгого префикса)? Ответ - ДА. Условия для проверки полуопределенности в духе Утверждения 5 являются сложными, поэтому мы предоставляем альтернативный метод проверки определенности, который вам хорошо известен из теории систем автоматического управления.\n",
    "\n",
    "> Утверждение 6. (Собственные значения и характер экстремума) Пусть $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ - собственные значения матрицы Гессе $H$. Матрица считается:\n",
    "> - положительно определенной, если все значения $\\lambda_i > 0$ для $i = 1, 2, \\ldots, n$,\n",
    "> - положительно полуопределенной, если все значения $\\lambda_i \\geq 0$ для $i = 1, 2, \\ldots, n$,\n",
    "> - отрицательно определенной, если все значения $\\lambda_i < 0$ для $i = 1, 2, \\ldots, n$,\n",
    "> - отрицательно полуопределенной, если все значения $\\lambda_i \\leq 0$ для $i = 1, 2, \\ldots, n$,\n",
    "> - неопределенной, если собственные значения изменяют знак. \n",
    "\n",
    "> Условие $\\leq i \\geq$ логически подразумевает, что существует хотя бы одно значение, отличное от нуля.\n",
    "\n",
    "### Линейная регрессия и метод наименьших квадратов\n",
    "\n",
    "Начнем с процедуры линейной регрессии, опираясь на метод наименьших квадратов (поскольку эти процедуры являются основой машинного обучения) и представим их как задачу оптимизации. С самого начала необходимо определить критерий оптимальности и представить его в квадратичной форме, где цель - минимизировать ошибку:\n",
    "\n",
    "$$ F = \\sum_{k=1}^{n} \\left( y(x_k) - y_k \\right)^2 \\tag{3}$$ \n",
    "\n",
    "Поскольку мы решаем проблему линейной регрессии, критерий оптимальности теперь принимает форму:\n",
    "\n",
    "$$ F = \\sum_{k=1}^{n} \\left( a_1x_k + a_0 - y_k \\right)^2 \\tag{4}$$\n",
    "\n",
    "Как мы ранее отметили, наша цель - определить оптимальные значения параметров $a_0$ и $a_1$, чтобы критерий оптимальности (2) был минимален. На основе необходимых условий экстремума получаем следующую систему уравнений:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "\\frac{\\partial F}{\\partial a_0} = \\sum_{k=1}^{n} 2(a_1x_k + a_0 - y_k) = 0 \\\\\n",
    "\\frac{\\partial F}{\\partial a_1} = \\sum_{k=1}^{n} 2(a_1x_k + a_0 - y_k)x_k = 0\n",
    "\\end{cases} \\tag{5}$$\n",
    "\n",
    "Линейная система уравнений (5) также может быть записана в матричной форме:\n",
    "\n",
    "$$ \\begin{bmatrix} n & \\sum_{k=1}^{n} x_k \\\\ \\sum_{k=1}^{n} x_k & \\sum_{k=1}^{n} x_k^2 \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} = \\begin{bmatrix} \\sum_{k=1}^{n} y_k \\\\ \\sum_{k=1}^{n} x_ky_k \\end{bmatrix} \\tag{6} $$\n",
    "\n",
    "Решим ее методом Краммера: \n",
    "\n",
    "Mетод Краммера представляет собой теорему в линейной алгебре, которая предоставляет решения системы линейных уравнений с использованием детерминант. Если систему уравнений представить в виде умножения матриц: $Ax = c$, где $A$ - квадратная матрица, $x$ - вектор-столбец переменных, и матрица $A$ является регулярной (невырожденной), то решение можно выразить следующим образом:\n",
    "\n",
    "$$x_i = \\frac{\\text{det}(A_i)}{\\text{det}(A)}$$\n",
    "\n",
    "где $A_i$ - матрица, полученная заменой $i$-го столбца в матрице $A$ вектором-столбцом $c$.\n",
    "\n",
    "В дальнейшем мы рассмотрим решение матричного уравнения (8) и его дальнейшее применение в изучении проблемы регрессии. Решение системы (6) получается с использованием правила Крамера. После решения системы уравнений и получения оптимальных значений параметров $a_0$ и $a_1$ формально завершается процесс линейной регрессии.\n",
    "\n",
    "$$ a_1 = \\frac{n \\sum_{k=1}^{n} x_k y_k - \\sum_{k=1}^{n} x_k \\sum_{k=1}^{n} y_k}{n \\sum_{k=1}^{n} x_k^2 - (\\sum_{k=1}^{n} x_k)^2} \\tag{7a}$$\n",
    "\n",
    "$$ a_0 = \\frac{\\sum_{k=1}^{n} y_k - a_1 \\sum_{k=1}^{n} x_k}{n} \\tag{7b}$$\n",
    "\n",
    "Где $a_0^*$ и $a_1^*$ обозначают оптимальные значения параметров $a_0$ и $a_1$ в соответствии с предполагаемым критерием оптимальности (2). Оптимальное значение критерия обозначим как $F_{LSE}$ и вычислим его следующим образом:\n",
    "\n",
    "$$ F_{LSE} = \\sum_{i=1}^{n} (a_1^*x_i + a_0^* - y_i)^2 \\tag{8} $$\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Оптимизация с ограничениями типа равенства, методы замены и ограниченные вариации\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Оптимизация с ограничениями типа равенства, метод множителей Лагранжа\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Оптимизация с ограничениями типа неравенства, введение дополнительной переменной методом замены, ограниченные вариации, множители Лагранжа\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Условия Каруша-Куна-Таккера (KKT)\n",
    "\n",
    "Условия Каруша-Куна-Таккера являются набором необходимых условий для оптимальности в задачах математического программирования с ограничениями типа неравенства и равенства. Представленные ниже условия широко используются для анализа оптимальности в задачах выпуклой оптимизации.\n",
    "\n",
    "1. **Необходимые условия стационарности:**\n",
    "   - Для точки оптимума $ \\mathbf{x}^* $ должно выполняться условие стационарности:\n",
    "     $$ \\nabla f(\\mathbf{x}^*) + \\sum \\lambda_i \\nabla g_i(\\mathbf{x}^*) + \\sum \\mu_j \\nabla h_j(\\mathbf{x}^*) = 0 $$\n",
    "     где $ \\lambda_i $ и $ \\mu_j $ - множители Лагранжа для ограничений типа неравенства и равенства соответственно.\n",
    "\n",
    "2. **Условия дополняющей нежесткости:**\n",
    "   - Для всех множителей Лагранжа $ \\lambda_i $ и $ \\mu_j $:\n",
    "     $$ \\lambda_i g_i(\\mathbf{x}^*) = 0 $$\n",
    "     $$ \\lambda_i \\geq 0 $$\n",
    "     $$ g_i(\\mathbf{x}^*) \\leq 0 $$\n",
    "     $$ h_j(\\mathbf{x}^*) = 0 $$\n",
    "\n",
    "3. **Условия исключения:**\n",
    "   - Если ограничение $ g_i(\\mathbf{x}^*) $ активно (то есть $ g_i(\\mathbf{x}^*) = 0 $), то соответствующий множитель Лагранжа $ \\lambda_i $ должен быть неотрицателен: $ \\lambda_i \\geq 0 $.\n",
    "\n",
    "4. **Дополнительные условия:**\n",
    "   - Для каждого ограничения типа неравенства $ g_i(\\mathbf{x}) \\leq 0 $, и множителя Лагранжа $ \\lambda_i $:\n",
    "     $$ \\lambda_i g_i(\\mathbf{x}^*) = 0 $$\n",
    "\n",
    "5. **Слабая устойчивость:**\n",
    "   - Матрица Якоби ограничений типа неравенства в активных точках оптимума должна быть полноранговой.\n",
    "\n",
    "Условия Каруша-Куна-Таккера предоставляют важные инсайты для понимания оптимальности в задачах с ограничениями и используются при разработке алгоритмов оптимизации и анализе решений.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Линейное программирование, графический метод, принципы Simplex метода, Simplex метод\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Численные методы для одномерных задач, градиентные методы, метод Ньютона-Рафсона, Метод Секущих\n",
    "\n",
    "**Основная идея этих методов:**\n",
    "Нахождение стационарных точек функции $(f'(x) = 0)$, при условии, что функция дифференцируема до необходимого для нас порядка.\n",
    "\n",
    "## **Метод Ньютона-Рапсона**\n",
    "**Метод Ньютона-Рапсона** — это численный метод решения уравнений, который использует итерационный процесс для приближенного нахождения корня уравнения. Алгоритм метода следующий:\n",
    "\n",
    "1. **Начальный выбор:**\n",
    "   - Выбирается начальное приближение $x_0$ к корню уравнения.\n",
    "\n",
    "2. **Итерационный процесс:**\n",
    "   - На каждом шаге $n$ вычисляется следующее приближение $x_{n+1}$ по формуле:\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}\n",
    "$$\n",
    "\n",
    "   где:\n",
    "   - $f'(x_n)$ — значение первой производной функции в точке $x_n$,\n",
    "   - $f''(x_n)$ — значение первой производной функции в точке $x_n$.\n",
    "\n",
    "3. **Обновление:**\n",
    "   - Процесс повторяется, и каждый раз вычисляется новое приближение корня.\n",
    "\n",
    "4. **Проверка сходимости:**\n",
    "   - Итерации продолжаются до тех пор, пока не будет достигнута заданная точность (критерий сходимости) или выполнено другое условие останова.\n",
    "\n",
    "Метод Ньютона-Рапсона имеет квадратичную скорость сходимости, что означает, что с каждой итерацией удваивается количество верных знаков в приближении к корню. Однако, он требует наличия производной функции, что может быть вызовом для сложных функций или в случае, когда вычисление производной затруднительно.\n",
    "\n",
    "## **Метод секущих**\n",
    "**Метод секущих (метод хорд)** — это численный метод решения уравнений, аппроксимирующий корень путем построения хорды (отрезка) на графике функции и использования ее для приближенного определения корня. \n",
    "\n",
    "Основное различие между методом секущих и методом Ньютона-Рафсона заключается в том, что в методе секущих вводится аппроксимация для второй производной:\n",
    "$$ f''(x_n) = \\frac{f'(x_n) - f'(x_{n-1})}{x_n - x_{n-1}} $$\n",
    "\n",
    "поэтому итерационная формула метода секущих принимает следующий вид:\n",
    "$$ x_{n+1} = x_n - f'(x_n)\\frac{x_n - x_{n-1}}{f'(x_n)-f'(x_{n-1})} $$\n",
    "\n",
    "Как видно из формулы, теперь не требуется, чтобы функция была дифференцируема до второго порядка, но теперь нам нужны две начальные точки. Выбором хорошего начального интервала (начальных точек) мы позволяем алгоритму сходиться к оптимуму. Этот подход может уменьшить скорость сходимости по сравнению с методом Ньютона-Рафсона.\n",
    "\n",
    "Алгоритм метода следующий:\n",
    "\n",
    "1. **Начальный выбор:**\n",
    "   - Выбираются две начальные точки $x_0$ и $x_1$, близкие к корню уравнения.\n",
    "\n",
    "2. **Итерационный процесс:**\n",
    "   - Строится хорда между точками $(x_n, f(x_n))$ и $(x_{n+1}, f(x_{n+1}))$.\n",
    "   - Новая аппроксимация корня $x_2$ находится как пересечение хорды с осью $x$.\n",
    "\n",
    "$$ x_{n+1} = x_n - f'(x_n)\\frac{x_n - x_{n-1}}{f'(x_n)-f'(x_{n-1})} $$\n",
    "\n",
    "3. **Обновление:**\n",
    "   - Точки $x_0$ и $x_1$ обновляются: $x_0 = x_1$, $x_1 = x_2$.\n",
    "\n",
    "4. **Повторение:**\n",
    "   - Процесс повторяется до сходимости (достаточного приближения к корню) или до выполнения другого критерия останова.\n",
    "\n",
    "Метод секущих аппроксимирует производную функции при помощи разностного приближения, и, в отличие от метода Ньютона, не требует вычисления производной в явной форме. Однако, он может быть менее быстрым в сходимости по сравнению с методом Ньютона.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Численные методы для одномерных задач, методы прямого поиска, метод Фибоначчи, метод золотого сечения\n",
    "\n",
    "**Методы прямого поиска в основном являются методами одномерной оптимизации.**\n",
    "Считаются \"скелетом\" для нелинейных алгоритмов оптимизации. Суть заключается в поиске на замкнутом интервале. Часто предполагаем, что функция унимодальна.\n",
    "\n",
    "**Унимодальная функция** - это функция, которая имеет только один максимум или минимум. Формально, функция $f(x)$ считается унимодальной на интервале, если существует такая точка $x^*$, что:\n",
    "\n",
    "1. Если $a < x^* < b$, то для всех $x$ в интервале $(a, x^*)$ функция $f(x)$ строго возрастает, и для всех $x$ в интервале $(x^*, b)$ функция $f(x)$ строго убывает.\n",
    "\n",
    "2. Если $a < b < x^*$, то для всех $x$ в интервале $(a, b)$ функция $f(x)$ строго возрастает.\n",
    "\n",
    "3. Если $x^* < a < b$, то для всех $x$ в интервале $(x^*, b)$ функция $f(x)$ строго убывает.\n",
    "\n",
    "Таким образом, унимодальная функция имеет только один пик (максимум или минимум) на заданном интервале. Важным свойством унимодальных функций является возможность применения эффективных методов одномерной оптимизации для их анализа и поиска экстремумов.\n",
    "\n",
    "## **Метод Фибоначчи**\n",
    "**Метод Фибоначчи** — это численный метод оптимизации, который используется для нахождения минимума (или максимума) функции в заданном интервале. Алгоритм метода включает следующие шаги:\n",
    "\n",
    "1. **Выбор начальных точек:**\n",
    "   - Задаются начальные границы интервала $[a, b]$ и точность $\\varepsilon$.\n",
    "\n",
    "2. **Инициализация ряда Фибоначчи:**\n",
    "   - Находится такое число $n$, что $F_n \\geq \\frac{b - a}{\\varepsilon}$, где $F_n$ — $n$-е число Фибоначчи.\n",
    "   - Выбираются начальные точки $x_1$ и $x_2$ внутри интервала так, чтобы длина интервала была равна $\\frac{b - a}{F_n}$.\n",
    "\n",
    "3. **Итерационный процесс:**\n",
    "   - На каждом шаге процесса точки $x_1$ и $x_2$ обновляются в зависимости от того, какая из подинтервалов $[a, x_2]$ или $[x_1, b]$ содержит минимум (максимум) функции.\n",
    "   - Длина выбранного подинтервала уменьшается в соответствии с последовательностью чисел Фибоначчи: $F_n$ умножается на $\\frac{F_{n-2}}{F_{n-1}}$.\n",
    "   - Процесс повторяется до достижения заданной точности.\n",
    "\n",
    "4. **Завершение:**\n",
    "   - Процесс завершается, когда длина текущего интервала становится меньше или равной $\\varepsilon$.\n",
    "\n",
    "Метод Фибоначчи основан на идее последовательного уточнения интервала, содержащего оптимальное решение, и может быть использован для оптимизации унимодальных функций. Однако, в сравнении с некоторыми другими методами оптимизации, метод Фибоначчи может требовать больше вычислительных ресурсов.\n",
    "\n",
    "## **Метод Золотого сечения**\n",
    "**Метод Золотого сечения** — это численный метод оптимизации, используемый для нахождения локального минимума (или максимума) унимодальной функции в заданном интервале. Алгоритм метода включает следующие шаги:\n",
    "\n",
    "1. **Выбор начальных точек:**\n",
    "   - Задаются начальные границы интервала $[a, b]$ и точность $\\varepsilon$.\n",
    "\n",
    "2. **Инициализация точек:**\n",
    "   - Выбираются две промежуточные точки $x_1$ и $x_2$ внутри интервала так, чтобы отношение длин отрезков $[a, x_2]$ и $[x_1, b]$ было равно золотому сечению, т.е.,\n",
    "\n",
    "$$\n",
    "\\frac{b - x_2}{x_2 - a} = \\frac{x_2 - a}{b - x_1} = \\phi\n",
    "$$\n",
    "\n",
    "где $\\phi$ — золотое сечение, приближенно равное 1.618.\n",
    "\n",
    "3. **Итерационный процесс:**\n",
    "   - Оценивается значение функции в точках $f(x_1)$ и $f(x_2)$.\n",
    "   - Сравниваются значения функции в точках $x_1$ и $x_2$, и один из концов интервала сужается в сторону, где функция принимает меньшее значение.\n",
    "   - Процесс повторяется, пока длина текущего интервала становится меньше или равной $\\varepsilon$.\n",
    "\n",
    "4. **Завершение:**\n",
    "   - Процесс завершается, когда достигается заданная точность.\n",
    "\n",
    "Метод Золотого сечения обладает свойством сходиться к оптимальному решению экспоненциально быстро. Этот метод особенно эффективен для унимодальных функций, т.е., функций, имеющих только один локальный экстремум.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Численные методы для многомерных задач, градиентные методы: метод наискорейшего спуска, метод наискорейшего спуска с фиксированным шагом\n",
    "## **Метод наискорейшего спуска**\n",
    "\n",
    "Этот метод используется для поиска минимума дифференцируемой функции \n",
    "$ f(x) = f(x_1, x_2, \\ldots, x_n) $, смещая текущее решение в направлении отрицательного градиента \n",
    "$ \\nabla f = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}]^T $ на каждой итерации.\n",
    "\n",
    "Метод состоит из следующих шагов:\n",
    "\n",
    "**Инициализация:** Выбираются начальное приближение $ x_0 $, размер шага $ \\gamma > 0 $, допустимая погрешность $ \\varepsilon > 0 $, и максимальное количество итераций $ N $.\n",
    "\n",
    "**Тело метода (итеративное применение):** На $ k $-ой итерации у нас есть\n",
    "$ x_{k+1} = x_k - \\gamma \\nabla f(x_k) $\n",
    "\n",
    "**Критерий остановки:** По окончании каждой итерации проверяем условие $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $. Когда это условие выполняется или когда мы достигаем максимального допустимого числа итераций, мы прекращаем выполнение алгоритма.\n",
    "\n",
    "$\\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Численные методы для многомерных задач, градиентные методы: метод наискорейшего спуска с моментом, ускоренный градиент Нестерова\n",
    "\n",
    "## **Градиентный метод с моментом**\n",
    "\n",
    "В методе градиентного спуска с моментом, общая структура алгоритма остается неизменной, но текущая позиция в процессе поиска обновляется немного измененным способом:\n",
    "\n",
    "$\\mathbf{v}_k = \\omega \\mathbf{v}_{k-1} + \\gamma \\nabla f(\\mathbf{x}_k)$\n",
    "\n",
    "$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\mathbf{v}_k$\n",
    "\n",
    "$\\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.\n",
    "\n",
    "$\\omega$ (коэффициент момента): Значение omega также обычно находится в диапазоне от 0.1 до 0.9. Можно начать с маленького значения, например, 0.1, и постепенно увеличивать, чтобы увидеть, как это влияет на производительность. Момент обычно помогает устранить осцилляции и ускорить сходимость.\n",
    "\n",
    "\n",
    "## **Ускоренный Градиент Нестерова**\n",
    "\n",
    "Основная идея ускоренного градиента Нестерова заключается в том, что градиент вычисляется в будущей точке.\n",
    "\n",
    "$\\mathbf{x}'_k = \\mathbf{x}_{k-1} - \\omega\\mathbf{v}_{k-1}$\n",
    "$\\mathbf{v}_k = \\omega \\mathbf{v}_{k-1} + \\gamma \\nabla f(\\mathbf{x}'_k)$ \n",
    "$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\mathbf{v}_k$\n",
    "Ключевым моментом в ускоренном градиенте Нестерова является то, что градиент вычисляется не в текущей точке, а в предполагаемой будущей точке. Таким образом, мы придаем всей процедуре определенный предсказательный характер, ожидая улучшения ее общего поведения.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Численные методы для многомерных задач, адаптивные градиентные методы: ADAGRAD, RMSProp, ADAM\n",
    "\n",
    "## **АДАГРАД**\n",
    "\n",
    "Adagrad использует адаптивный градиент, специфичный для каждой оси (каждой переменной).\n",
    "\n",
    "Пусть $ g_{k, i} $ - градиент критерия оптимальности по $ i $-й переменной в $ k $-й итерации,\n",
    "\n",
    "### $ G_{k, i} = \\sum_{j=1}^{k} (g_{j, i})^2 $\n",
    "\n",
    "где $ G_{k, i} $ - сумма квадратов градиентов по $ i $-й переменной до $ k $-й итерации.\n",
    "\n",
    "Обновление $ i $-й переменной:\n",
    "\n",
    "### $ x_{k+1, i} = x_{k, i} - \\frac{\\gamma}{\\sqrt{G_{k, i} + \\epsilon}} g_{k, i} $\n",
    "\n",
    "где $ \\gamma $ - скорость обучения, $ \\epsilon $ - малая константа, предотвращающая деление на ноль.\n",
    "\n",
    "Этот метод позволяет каждой переменной адаптивно регулировать скорость обучения, учитывая её историю градиентов.\n",
    "\n",
    "## **RMSProp**\n",
    "\n",
    "Алгоритм RMSProp работает аналогично ADAGRAD, за исключением того, что квадраты градиента не накапливаются бесконечно. Вместо этого вводится процедура, которая поверхностно напоминает процедуру введения момента в градиентном алгоритме.\n",
    "\n",
    "### $ G_{k+1, i} = \\omega G_{k, i} + (1 - \\omega) g_{k, i}^2  $\n",
    "\n",
    "Типичное значение параметра $ \\omega $ - 0.9.\n",
    "\n",
    "Предположим, что $ g^2 $ постоянно. Когда выражение выше сходится, значение $ G $ в установившемся состоянии будет\n",
    "### $ G = \\omega G + (1 - \\omega) g^2 $\n",
    "Иными словами, $ G = g^2 $\n",
    "\n",
    "\n",
    "## **ADAM**\n",
    "\n",
    "ADAM (ADAPTIVE MOMENT ESTIMATION) - одна из наиболее широко используемых современных модификаций алгоритма наискорейшего спуска.\n",
    "\n",
    "Сначала определяются вспомогательные величины:\n",
    "\n",
    "### $ m_k = \\omega_1 m_{k-1} + (1 - \\omega_1) g_k $\n",
    "### $ v_k = \\omega_2 v_{k-1} + (1 - \\omega_2) g_{k}^2 $\n",
    "\n",
    "и их скорректированные версии:\n",
    "\n",
    "### $ \\hat{m}_k = \\frac{m_k}{1 - \\omega_1^k} $\n",
    "### $ \\hat{v}_k = \\frac{v_k}{1 - \\omega_2^k} $\n",
    "\n",
    "Затем текущее решение обновляется по алгоритму:\n",
    "\n",
    "### $$ x_{k+1} = x_k - \\frac{\\gamma}{\\sqrt{\\hat{v}_k + \\epsilon}} \\hat{m}_k $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Принципы оптимизации на основе генетического алгоритма. Описание реализации алгоритма с бинарно закодированными особями.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Принципы оптимизации на основе генетического алгоритма. Описание реализации алгоритма с вещественно закодированными особями.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Принципы оптимизации на основе PSO алгоритма\n",
    "\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
