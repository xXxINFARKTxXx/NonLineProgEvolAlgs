{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Список вопросов для экзамена по оптимизации 2021-2022 учебный год"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Одномерная оптимизация: Необходимые и достаточные условия\n",
    "\n",
    "Одномерная оптимизация означает поиск оптимального значения для функции одной переменной. Ниже представлены необходимые и достаточные условия для точки экстремума в одномерной оптимизации:\n",
    "\n",
    "### Необходимые условия:\n",
    "\n",
    "1. **Стационарная точка:** Если функция $ f(x) $ имеет локальный экстремум в точке $ x = x^* $, то производная функции в этой точке должна быть равна нулю: \n",
    "   $$ f'(x^*) = 0 $$\n",
    "\n",
    "### Достаточные условия:\n",
    "\n",
    "1. **Вторая производная:** Проверка знака второй производной в точке $ x^* $ позволяет определить характер экстремума:\n",
    "   - Если $ f''(x^*) > 0 $, то это точка локального минимума.\n",
    "   - Если $ f''(x^*) < 0 $, то это точка локального максимума.\n",
    "\n",
    "2. **Тест первой производной (знак изменения производной):**\n",
    "   - Если $ f'(x) $ меняет знак с положительного на отрицательный при переходе через точку $ x^* $, то $ x^* $ - локальный максимум.\n",
    "   - Если $ f'(x) $ меняет знак с отрицательного на положительный при переходе через точку $ x^* $, то $ x^* $ - локальный минимум.\n",
    "\n",
    "Эти условия позволяют анализировать свойства точек экстремума в одномерной оптимизации.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Многомерная оптимизация без ограничений\n",
    "### Разложение в ряд Тейлора\n",
    "\n",
    "Развитие функции многих переменных в ряд Тейлора\n",
    "\n",
    "Рассматриваем функцию многих переменных $y(x) = y(x_1, x_2, x_3, . . . , x_n )$, которая представляет собой критерий оптимальности, чей минимум/максимум мы пытаемся определить. Здесь $x$ - вектор переменных состояния, обозначенный и определенный как $x = [x_1, x_2, x_3, . . . , x_n ]^T$.\n",
    "\n",
    "Предполагая, что функция (1) определена и непрерывна вместе со своими $j + 1$ производными в окрестности точки $x^*$, тогда обычно разложение в ряд Тейлора в его общей форме записывается как:\n",
    "\n",
    "$$y(x) = y(x^*) + \\frac{\\Delta y}{1!} + \\frac{\\Delta^2 y}{2!} + . . . + \\frac{\\Delta^j y}{j!} + R_{j+1}$$\n",
    "\n",
    "где $R_{j+1}$ - остаток разложения в ряд, а остальные члены, вместе с аналогичным обобщением на более высокие производные, представлены следующим образом:\n",
    "\n",
    "$$\\Delta y = (x_1 - x^*_1)\\frac{\\partial y}{\\partial x_1}\\bigg|_{x^*} + (x_2 - x^*_2)\\frac{\\partial y}{\\partial x_2}\\bigg|_{x^*} + . . . + (x_n - x^*_n)\\frac{\\partial y}{\\partial x_n}\\bigg|_{x^*}$$\n",
    "\n",
    "$$\\Delta^2 y = (x_1 - x^*_1)^2\\frac{\\partial^2 y}{\\partial x_1^2}\\bigg|_{x^*} + 2(x_1 - x^*_1)(x_2 - x^*_2)\\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}\\bigg|_{x^*} + (x_2 - x^*_2)^2\\frac{\\partial^2 y}{\\partial x_2^2}\\bigg|_{x^*} + . . . + (x_n - x^*_n)^2\\frac{\\partial^2 y}{\\partial x_n^2}\\bigg|_{x^*}$$\n",
    "\n",
    "(и так далее для более высоких порядков).\n",
    "\n",
    "В общем виде:\n",
    "\n",
    "$$\n",
    "\\Delta^k y = \\sum_{p_1, p_2, ..., p_n} \\frac{k!}{p_1! p_2! ... p_n!} (x_1 - x^*_1)^{p_1} (x_2 - x^*_2)^{p_2} ... (x_n - x^*_n)^{p_n} \\frac{\\partial^k y}{\\partial x^{p_1}_1 \\partial x^{p_2}_2 ... \\partial x^{p_n}_n}\\bigg|_{x^*}\n",
    "$$\n",
    "\n",
    "Здесь $p_1, p_2, ..., p_n$ - неотрицательные целые числа, и суммирование происходит по всем возможным наборам $p_1, p_2, ..., p_n$ таким, что $p_1 + p_2 + ... + p_n = k$.\n",
    "\n",
    "Тогда для функции двух переменных $y(\\hat{x}) = y(x1, x2)$:\n",
    "$$\n",
    "\\Delta y = (x_1 - x^*_1)\\frac{\\partial y}{\\partial x_1}\\bigg|_{x^*} + (x_2 - x^*_2)\\frac{\\partial y}{\\partial x_2}\\bigg|_{x^*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta^2 y = (x_1 - x^*_1)^2\\frac{\\partial^2 y}{\\partial x_1^2}\\bigg|_{x^*} + 2(x_1 - x^*_1)(x_2 - x^*_2)\\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}\\bigg|_{x^*} + (x_2 - x^*_2)^2\\frac{\\partial^2 y}{\\partial x_2^2}\\bigg|_{x^*}\n",
    "$$\n",
    "\n",
    "Применяя рассуждения из одномерной оптимизации, мы можем ввести замену $x_i - x^*_i = h_i$ и упростить выражения (6), записав их в более компактной форме, более удобной для дальнейшего анализа:\n",
    "\n",
    "$$\n",
    "\\Delta y = h_1 \\frac{\\partial y}{\\partial x_1}\\bigg|_{x^*} + h_2 \\frac{\\partial y}{\\partial x_2}\\bigg|_{x^*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta^2 y = h_1^2 \\frac{\\partial^2 y}{\\partial x_1^2}\\bigg|_{x^*} + 2h_1 h_2 \\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}\\bigg|_{x^*} + h_2^2 \\frac{\\partial^2 y}{\\partial x_2^2}\\bigg|_{x^*}\n",
    "$$\n",
    "\n",
    "Необходимые и достаточные условия для экстремума функции двух переменных напоминают случай функции одной переменной. Напоминаем выражение для разницы функции одной переменной:\n",
    "\n",
    "$$y(x^* + h) - y(x^*) = h y'(x^*) + \\frac{h^2}{2} y''(x^*)$$\n",
    "\n",
    "Здесь, из-за колебаний знака $h$, мы вводим необходимое условие того, что первая производная в точке $x^*$ равна нулю, то есть $y'(x^*) = 0$. Аналогичное рассуждение применяется и для функции двух переменных.\n",
    "\n",
    "В случае функции нескольких переменных, сводится к условию, что все частные производные по переменным $x_i$ равны нулю. То есть:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial y}{\\partial x_i}\\Bigg|_{x^*} = 0 \\text{ для всех i = 1, 2}  \n",
    "$$.\n",
    "\n",
    "Все кандидаты на (локальный) минимум/максимум находятся среди стационарных точек. Это утверждение можно сформулировать следующим интуитивным образом.\n",
    "\n",
    "> **Утверждение 1. (Необходимые условия экстремума функции двух переменных):** В случае, если существуют частные производные первого порядка функции $y$ по всем переменным, то точка экстремума функции $y$ в точке $x^*$ удовлетворяет системе уравнений $$ \\frac{\\partial y}{\\partial x_i}\\Bigg|_{x^*} = 0 \\quad \\text{ для } i = 1, 2 $$ или условию $$ \\Delta y = 0 $$.\n",
    "\n",
    "Если точка $ (x^*_1, x^*_2) $ удовлетворяет условиям стационарности (необходимые условия экстремума), то она считается стационарной точкой. Тогда, необходимые условия для экстремума могут быть выражены следующим образом:\n",
    "\n",
    "$$ y(x) - y(x^*) = \\frac{\\Delta^2 y}{2!} $$\n",
    "\n",
    "где $ \\Delta^2 y $ представляет собой изменение производной функции второго порядка, а $ y(x^*) $ - значение функции в стационарной точке $ (x^*_1, x^*_2) $.\n",
    "\n",
    "Далее, из предыдущего выражения ясно видно, что стационарная точка $ (x^*_1, x^*_2) $ имеет максимальное значение, если правая часть уравнения меньше нуля, то есть минимальное значение достигается при $ \\Delta^2 y < 0 $. Предположим, что нас интересует, когда функция (5) достигает максимального значения, то есть когда выражение $ \\Delta^2 y < 0 $:\n",
    "\n",
    "$$ \\Delta^2 y = h^2_{1} \\frac{\\partial^2 y}{\\partial x_{1}^2} \\Bigr|_{\\ast} + 2h_{1}h_{2} \\frac{\\partial^2 y}{\\partial x_{1} \\partial x_{2}} \\Bigr|_{\\ast} + h^2_{2} \\frac{\\partial^2 y}{\\partial x_{2}^2} \\Bigr|_{\\ast} < 0 \\text{ при нетривиальных } h_1 \\text{ и } h_2 \\tag{11}$$\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y > 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ положительно определена.\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y \\leq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ отрицательно полуопределена.\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y \\geq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ положительно полуопределена.\n",
    "\n",
    "> Если значение функции $ \\Delta^2 y \\neq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \\Delta^2 $ неопределена.\n",
    "\n",
    "\n",
    "Если условие (11) выполняется, мы утверждаем, что функция $ \\Delta^2 y $ отрицательно определена. Обычно в литературе понятие определенности связано с концепциями минимума и максимума, а процесс выведения достаточных условий называется исследованием определенности. Как вы увидите, это не тривиальная задача и, вероятно, является наиболее сложной в этой части курса.\n",
    "\n",
    "У нашем конкретном случае, наша цель - вывести достаточные условия, при которых справедливы соотношения (10) и (11). Это сводится к тому, чтобы представить эти уравнения в компактной форме, в которой все члены $h_1$ и $h_2$ будут под знаком квадрата, что гарантирует инвариантность условий относительно знака параметров $h_i$.\n",
    "\n",
    "В процессе вывода, который следует, мы следуем логике, которая приведет нас к условиям, гарантирующим отрицательную (положительную) определенность. Другие граничные случаи мы рассмотрим через примеры, и они не являются предметом данного вопроса. Выражение (10) мы можем разложить следующим образом:\n",
    "\n",
    "$$ \n",
    "\\Delta^2 y = \n",
    "\\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "\\left[\n",
    "    h^2_{1} \n",
    "    +\n",
    "    2h_1 h_2 \\frac{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}\\partial x_{2}} \\right)^*\n",
    "    }{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "    }\n",
    "    +\n",
    "    h^2_2 \\frac{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^*\n",
    "    }{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "    }\n",
    "\\right] < 0\n",
    "$$\n",
    "\n",
    "\n",
    "Первые два члена в скобках можно рассматривать как часть бинома, а остальные члены мы сгруппируем отдельно. В выражении (13) мы достигли того, что все члены $h_i$ теперь под знаком квадрата, и их вклад в характер экстремума теперь не вызывает сомнений. Таким образом, на определенность теперь влияют только члены, для которых мы предполагаем, что они меньше 0.\n",
    "\n",
    "$$ \n",
    "\\Delta^2 y = \n",
    "\\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "\\left[\n",
    "    h^2_{1} \n",
    "    +\n",
    "    2h_1 h_2 \\frac{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}\\partial x_{2}} \\right)^*\n",
    "    }{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "    }\n",
    "\\right] ^ 2\n",
    "+\n",
    "h^2_2 \\left[\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^*\n",
    "    - \n",
    "    \\frac{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^{*2}\n",
    "    }{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "    }\n",
    "\\right] < 0\n",
    "$$\n",
    "\n",
    "В выражении (13) мы добились того, что все члены $h_i$ теперь под знаком квадрата, и их вклад в характер экстремума теперь не вызывает сомнений, то есть на определенность теперь влияют только члены, для которых мы предполагаем, что они меньше 0.\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^* < 0 \n",
    "\\text{  и  }\n",
    "\\left[\n",
    "    \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^*\n",
    "    - \n",
    "    \\frac{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{2}^2} \\right)^{*2}\n",
    "    }{\n",
    "        \\left( \\frac{\\partial^2 y}{\\partial x_{1}^2} \\right)^*\n",
    "    }\n",
    "\\right] < 0\n",
    "$$\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Оптимизация с ограничениями типа равенства, методы замены и ограниченные вариации\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Оптимизация с ограничениями типа равенства, метод множителей Лагранжа\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Оптимизация с ограничениями типа неравенства, введение дополнительной переменной методом замены, ограниченные вариации, множители Лагранжа\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Условия Каруша-Куна-Таккера (KKT)\n",
    "\n",
    "Условия Каруша-Куна-Таккера являются набором необходимых условий для оптимальности в задачах математического программирования с ограничениями типа неравенства и равенства. Представленные ниже условия широко используются для анализа оптимальности в задачах выпуклой оптимизации.\n",
    "\n",
    "1. **Необходимые условия стационарности:**\n",
    "   - Для точки оптимума $ \\mathbf{x}^* $ должно выполняться условие стационарности:\n",
    "     $$ \\nabla f(\\mathbf{x}^*) + \\sum \\lambda_i \\nabla g_i(\\mathbf{x}^*) + \\sum \\mu_j \\nabla h_j(\\mathbf{x}^*) = 0 $$\n",
    "     где $ \\lambda_i $ и $ \\mu_j $ - множители Лагранжа для ограничений типа неравенства и равенства соответственно.\n",
    "\n",
    "2. **Условия дополняющей нежесткости:**\n",
    "   - Для всех множителей Лагранжа $ \\lambda_i $ и $ \\mu_j $:\n",
    "     $$ \\lambda_i g_i(\\mathbf{x}^*) = 0 $$\n",
    "     $$ \\lambda_i \\geq 0 $$\n",
    "     $$ g_i(\\mathbf{x}^*) \\leq 0 $$\n",
    "     $$ h_j(\\mathbf{x}^*) = 0 $$\n",
    "\n",
    "3. **Условия исключения:**\n",
    "   - Если ограничение $ g_i(\\mathbf{x}^*) $ активно (то есть $ g_i(\\mathbf{x}^*) = 0 $), то соответствующий множитель Лагранжа $ \\lambda_i $ должен быть неотрицателен: $ \\lambda_i \\geq 0 $.\n",
    "\n",
    "4. **Дополнительные условия:**\n",
    "   - Для каждого ограничения типа неравенства $ g_i(\\mathbf{x}) \\leq 0 $, и множителя Лагранжа $ \\lambda_i $:\n",
    "     $$ \\lambda_i g_i(\\mathbf{x}^*) = 0 $$\n",
    "\n",
    "5. **Слабая устойчивость:**\n",
    "   - Матрица Якоби ограничений типа неравенства в активных точках оптимума должна быть полноранговой.\n",
    "\n",
    "Условия Каруша-Куна-Таккера предоставляют важные инсайты для понимания оптимальности в задачах с ограничениями и используются при разработке алгоритмов оптимизации и анализе решений.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Линейное программирование, графический метод, принципы Simplex метода, Simplex метод\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Численные методы для одномерных задач, градиентные методы, метод Ньютона-Рафсона, Метод Секущих\n",
    "\n",
    "**Основная идея этих методов:**\n",
    "Нахождение стационарных точек функции $(f'(x) = 0)$, при условии, что функция дифференцируема до необходимого для нас порядка.\n",
    "\n",
    "## **Метод Ньютона-Рапсона**\n",
    "**Метод Ньютона-Рапсона** — это численный метод решения уравнений, который использует итерационный процесс для приближенного нахождения корня уравнения. Алгоритм метода следующий:\n",
    "\n",
    "1. **Начальный выбор:**\n",
    "   - Выбирается начальное приближение $x_0$ к корню уравнения.\n",
    "\n",
    "2. **Итерационный процесс:**\n",
    "   - На каждом шаге $n$ вычисляется следующее приближение $x_{n+1}$ по формуле:\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}\n",
    "$$\n",
    "\n",
    "   где:\n",
    "   - $f'(x_n)$ — значение первой производной функции в точке $x_n$,\n",
    "   - $f''(x_n)$ — значение первой производной функции в точке $x_n$.\n",
    "\n",
    "3. **Обновление:**\n",
    "   - Процесс повторяется, и каждый раз вычисляется новое приближение корня.\n",
    "\n",
    "4. **Проверка сходимости:**\n",
    "   - Итерации продолжаются до тех пор, пока не будет достигнута заданная точность (критерий сходимости) или выполнено другое условие останова.\n",
    "\n",
    "Метод Ньютона-Рапсона имеет квадратичную скорость сходимости, что означает, что с каждой итерацией удваивается количество верных знаков в приближении к корню. Однако, он требует наличия производной функции, что может быть вызовом для сложных функций или в случае, когда вычисление производной затруднительно.\n",
    "\n",
    "## **Метод секущих**\n",
    "**Метод секущих (метод хорд)** — это численный метод решения уравнений, аппроксимирующий корень путем построения хорды (отрезка) на графике функции и использования ее для приближенного определения корня. \n",
    "\n",
    "Основное различие между методом секущих и методом Ньютона-Рафсона заключается в том, что в методе секущих вводится аппроксимация для второй производной:\n",
    "$$ f''(x_n) = \\frac{f'(x_n) - f'(x_{n-1})}{x_n - x_{n-1}} $$\n",
    "\n",
    "поэтому итерационная формула метода секущих принимает следующий вид:\n",
    "$$ x_{n+1} = x_n - f'(x_n)\\frac{x_n - x_{n-1}}{f'(x_n)-f'(x_{n-1})} $$\n",
    "\n",
    "Как видно из формулы, теперь не требуется, чтобы функция была дифференцируема до второго порядка, но теперь нам нужны две начальные точки. Выбором хорошего начального интервала (начальных точек) мы позволяем алгоритму сходиться к оптимуму. Этот подход может уменьшить скорость сходимости по сравнению с методом Ньютона-Рафсона.\n",
    "\n",
    "Алгоритм метода следующий:\n",
    "\n",
    "1. **Начальный выбор:**\n",
    "   - Выбираются две начальные точки $x_0$ и $x_1$, близкие к корню уравнения.\n",
    "\n",
    "2. **Итерационный процесс:**\n",
    "   - Строится хорда между точками $(x_n, f(x_n))$ и $(x_{n+1}, f(x_{n+1}))$.\n",
    "   - Новая аппроксимация корня $x_2$ находится как пересечение хорды с осью $x$.\n",
    "\n",
    "$$ x_{n+1} = x_n - f'(x_n)\\frac{x_n - x_{n-1}}{f'(x_n)-f'(x_{n-1})} $$\n",
    "\n",
    "3. **Обновление:**\n",
    "   - Точки $x_0$ и $x_1$ обновляются: $x_0 = x_1$, $x_1 = x_2$.\n",
    "\n",
    "4. **Повторение:**\n",
    "   - Процесс повторяется до сходимости (достаточного приближения к корню) или до выполнения другого критерия останова.\n",
    "\n",
    "Метод секущих аппроксимирует производную функции при помощи разностного приближения, и, в отличие от метода Ньютона, не требует вычисления производной в явной форме. Однако, он может быть менее быстрым в сходимости по сравнению с методом Ньютона.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Численные методы для одномерных задач, методы прямого поиска, метод Фибоначчи, метод золотого сечения\n",
    "\n",
    "**Методы прямого поиска в основном являются методами одномерной оптимизации.**\n",
    "Считаются \"скелетом\" для нелинейных алгоритмов оптимизации. Суть заключается в поиске на замкнутом интервале. Часто предполагаем, что функция унимодальна.\n",
    "\n",
    "**Унимодальная функция** - это функция, которая имеет только один максимум или минимум. Формально, функция $f(x)$ считается унимодальной на интервале, если существует такая точка $x^*$, что:\n",
    "\n",
    "1. Если $a < x^* < b$, то для всех $x$ в интервале $(a, x^*)$ функция $f(x)$ строго возрастает, и для всех $x$ в интервале $(x^*, b)$ функция $f(x)$ строго убывает.\n",
    "\n",
    "2. Если $a < b < x^*$, то для всех $x$ в интервале $(a, b)$ функция $f(x)$ строго возрастает.\n",
    "\n",
    "3. Если $x^* < a < b$, то для всех $x$ в интервале $(x^*, b)$ функция $f(x)$ строго убывает.\n",
    "\n",
    "Таким образом, унимодальная функция имеет только один пик (максимум или минимум) на заданном интервале. Важным свойством унимодальных функций является возможность применения эффективных методов одномерной оптимизации для их анализа и поиска экстремумов.\n",
    "\n",
    "## **Метод Фибоначчи**\n",
    "**Метод Фибоначчи** — это численный метод оптимизации, который используется для нахождения минимума (или максимума) функции в заданном интервале. Алгоритм метода включает следующие шаги:\n",
    "\n",
    "1. **Выбор начальных точек:**\n",
    "   - Задаются начальные границы интервала $[a, b]$ и точность $\\varepsilon$.\n",
    "\n",
    "2. **Инициализация ряда Фибоначчи:**\n",
    "   - Находится такое число $n$, что $F_n \\geq \\frac{b - a}{\\varepsilon}$, где $F_n$ — $n$-е число Фибоначчи.\n",
    "   - Выбираются начальные точки $x_1$ и $x_2$ внутри интервала так, чтобы длина интервала была равна $\\frac{b - a}{F_n}$.\n",
    "\n",
    "3. **Итерационный процесс:**\n",
    "   - На каждом шаге процесса точки $x_1$ и $x_2$ обновляются в зависимости от того, какая из подинтервалов $[a, x_2]$ или $[x_1, b]$ содержит минимум (максимум) функции.\n",
    "   - Длина выбранного подинтервала уменьшается в соответствии с последовательностью чисел Фибоначчи: $F_n$ умножается на $\\frac{F_{n-2}}{F_{n-1}}$.\n",
    "   - Процесс повторяется до достижения заданной точности.\n",
    "\n",
    "4. **Завершение:**\n",
    "   - Процесс завершается, когда длина текущего интервала становится меньше или равной $\\varepsilon$.\n",
    "\n",
    "Метод Фибоначчи основан на идее последовательного уточнения интервала, содержащего оптимальное решение, и может быть использован для оптимизации унимодальных функций. Однако, в сравнении с некоторыми другими методами оптимизации, метод Фибоначчи может требовать больше вычислительных ресурсов.\n",
    "\n",
    "## **Метод Золотого сечения**\n",
    "**Метод Золотого сечения** — это численный метод оптимизации, используемый для нахождения локального минимума (или максимума) унимодальной функции в заданном интервале. Алгоритм метода включает следующие шаги:\n",
    "\n",
    "1. **Выбор начальных точек:**\n",
    "   - Задаются начальные границы интервала $[a, b]$ и точность $\\varepsilon$.\n",
    "\n",
    "2. **Инициализация точек:**\n",
    "   - Выбираются две промежуточные точки $x_1$ и $x_2$ внутри интервала так, чтобы отношение длин отрезков $[a, x_2]$ и $[x_1, b]$ было равно золотому сечению, т.е.,\n",
    "\n",
    "$$\n",
    "\\frac{b - x_2}{x_2 - a} = \\frac{x_2 - a}{b - x_1} = \\phi\n",
    "$$\n",
    "\n",
    "где $\\phi$ — золотое сечение, приближенно равное 1.618.\n",
    "\n",
    "3. **Итерационный процесс:**\n",
    "   - Оценивается значение функции в точках $f(x_1)$ и $f(x_2)$.\n",
    "   - Сравниваются значения функции в точках $x_1$ и $x_2$, и один из концов интервала сужается в сторону, где функция принимает меньшее значение.\n",
    "   - Процесс повторяется, пока длина текущего интервала становится меньше или равной $\\varepsilon$.\n",
    "\n",
    "4. **Завершение:**\n",
    "   - Процесс завершается, когда достигается заданная точность.\n",
    "\n",
    "Метод Золотого сечения обладает свойством сходиться к оптимальному решению экспоненциально быстро. Этот метод особенно эффективен для унимодальных функций, т.е., функций, имеющих только один локальный экстремум.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Численные методы для многомерных задач, градиентные методы: метод наискорейшего спуска, метод наискорейшего спуска с фиксированным шагом\n",
    "## **Метод наискорейшего спуска**\n",
    "\n",
    "Этот метод используется для поиска минимума дифференцируемой функции \n",
    "$ f(x) = f(x_1, x_2, \\ldots, x_n) $, смещая текущее решение в направлении отрицательного градиента \n",
    "$ \\nabla f = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}]^T $ на каждой итерации.\n",
    "\n",
    "Метод состоит из следующих шагов:\n",
    "\n",
    "**Инициализация:** Выбираются начальное приближение $ x_0 $, размер шага $ \\gamma > 0 $, допустимая погрешность $ \\varepsilon > 0 $, и максимальное количество итераций $ N $.\n",
    "\n",
    "**Тело метода (итеративное применение):** На $ k $-ой итерации у нас есть\n",
    "$ x_{k+1} = x_k - \\gamma \\nabla f(x_k) $\n",
    "\n",
    "**Критерий остановки:** По окончании каждой итерации проверяем условие $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $. Когда это условие выполняется или когда мы достигаем максимального допустимого числа итераций, мы прекращаем выполнение алгоритма.\n",
    "\n",
    "$\\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Численные методы для многомерных задач, градиентные методы: метод наискорейшего спуска с моментом, ускоренный градиент Нестерова\n",
    "\n",
    "## **Градиентный метод с моментом**\n",
    "\n",
    "В методе градиентного спуска с моментом, общая структура алгоритма остается неизменной, но текущая позиция в процессе поиска обновляется немного измененным способом:\n",
    "\n",
    "$\\mathbf{v}_k = \\omega \\mathbf{v}_{k-1} + \\gamma \\nabla f(\\mathbf{x}_k)$\n",
    "\n",
    "$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\mathbf{v}_k$\n",
    "\n",
    "$\\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.\n",
    "\n",
    "$\\omega$ (коэффициент момента): Значение omega также обычно находится в диапазоне от 0.1 до 0.9. Можно начать с маленького значения, например, 0.1, и постепенно увеличивать, чтобы увидеть, как это влияет на производительность. Момент обычно помогает устранить осцилляции и ускорить сходимость.\n",
    "\n",
    "\n",
    "## **Ускоренный Градиент Нестерова**\n",
    "\n",
    "Основная идея ускоренного градиента Нестерова заключается в том, что градиент вычисляется в будущей точке.\n",
    "\n",
    "$\\mathbf{x}'_k = \\mathbf{x}_{k-1} - \\omega\\mathbf{v}_{k-1}$\n",
    "$\\mathbf{v}_k = \\omega \\mathbf{v}_{k-1} + \\gamma \\nabla f(\\mathbf{x}'_k)$ \n",
    "$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\mathbf{v}_k$\n",
    "Ключевым моментом в ускоренном градиенте Нестерова является то, что градиент вычисляется не в текущей точке, а в предполагаемой будущей точке. Таким образом, мы придаем всей процедуре определенный предсказательный характер, ожидая улучшения ее общего поведения.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Численные методы для многомерных задач, адаптивные градиентные методы: ADAGRAD, RMSProp, ADAM\n",
    "\n",
    "## **АДАГРАД**\n",
    "\n",
    "Adagrad использует адаптивный градиент, специфичный для каждой оси (каждой переменной).\n",
    "\n",
    "Пусть $ g_{k, i} $ - градиент критерия оптимальности по $ i $-й переменной в $ k $-й итерации,\n",
    "\n",
    "### $ G_{k, i} = \\sum_{j=1}^{k} (g_{j, i})^2 $\n",
    "\n",
    "где $ G_{k, i} $ - сумма квадратов градиентов по $ i $-й переменной до $ k $-й итерации.\n",
    "\n",
    "Обновление $ i $-й переменной:\n",
    "\n",
    "### $ x_{k+1, i} = x_{k, i} - \\frac{\\gamma}{\\sqrt{G_{k, i} + \\epsilon}} g_{k, i} $\n",
    "\n",
    "где $ \\gamma $ - скорость обучения, $ \\epsilon $ - малая константа, предотвращающая деление на ноль.\n",
    "\n",
    "Этот метод позволяет каждой переменной адаптивно регулировать скорость обучения, учитывая её историю градиентов.\n",
    "\n",
    "## **RMSProp**\n",
    "\n",
    "Алгоритм RMSProp работает аналогично ADAGRAD, за исключением того, что квадраты градиента не накапливаются бесконечно. Вместо этого вводится процедура, которая поверхностно напоминает процедуру введения момента в градиентном алгоритме.\n",
    "\n",
    "### $ G_{k+1, i} = \\omega G_{k, i} + (1 - \\omega) g_{k, i}^2  $\n",
    "\n",
    "Типичное значение параметра $ \\omega $ - 0.9.\n",
    "\n",
    "Предположим, что $ g^2 $ постоянно. Когда выражение выше сходится, значение $ G $ в установившемся состоянии будет\n",
    "### $ G = \\omega G + (1 - \\omega) g^2 $\n",
    "Иными словами, $ G = g^2 $\n",
    "\n",
    "\n",
    "## **ADAM**\n",
    "\n",
    "ADAM (ADAPTIVE MOMENT ESTIMATION) - одна из наиболее широко используемых современных модификаций алгоритма наискорейшего спуска.\n",
    "\n",
    "Сначала определяются вспомогательные величины:\n",
    "\n",
    "### $ m_k = \\omega_1 m_{k-1} + (1 - \\omega_1) g_k $\n",
    "### $ v_k = \\omega_2 v_{k-1} + (1 - \\omega_2) g_{k}^2 $\n",
    "\n",
    "и их скорректированные версии:\n",
    "\n",
    "### $ \\hat{m}_k = \\frac{m_k}{1 - \\omega_1^k} $\n",
    "### $ \\hat{v}_k = \\frac{v_k}{1 - \\omega_2^k} $\n",
    "\n",
    "Затем текущее решение обновляется по алгоритму:\n",
    "\n",
    "### $$ x_{k+1} = x_k - \\frac{\\gamma}{\\sqrt{\\hat{v}_k + \\epsilon}} \\hat{m}_k $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Принципы оптимизации на основе генетического алгоритма. Описание реализации алгоритма с бинарно закодированными особями.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Принципы оптимизации на основе генетического алгоритма. Описание реализации алгоритма с вещественно закодированными особями.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Принципы оптимизации на основе PSO алгоритма\n",
    "\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
