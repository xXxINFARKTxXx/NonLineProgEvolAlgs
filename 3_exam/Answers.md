# Список вопросов для экзамена по оптимизации 2021-2022 учебный год

## 1. Одномерная оптимизация: Необходимые и достаточные условия

Одномерная оптимизация означает поиск оптимального значения для функции одной переменной. Ниже представлены необходимые и достаточные условия для точки экстремума в одномерной оптимизации:

### Необходимые условия:

1. **Стационарная точка:** Если функция $ f(x) $ имеет локальный экстремум в точке $ x = x^* $, то производная функции в этой точке должна быть равна нулю: 
   $$ f'(x^*) = 0 $$

### Достаточные условия:

1. **Вторая производная:** Проверка знака второй производной в точке $ x^* $ позволяет определить характер экстремума:
   - Если $ f''(x^*) > 0 $, то это точка локального минимума.
   - Если $ f''(x^*) < 0 $, то это точка локального максимума.

2. **Тест первой производной (знак изменения производной):**
   - Если $ f'(x) $ меняет знак с положительного на отрицательный при переходе через точку $ x^* $, то $ x^* $ - локальный максимум.
   - Если $ f'(x) $ меняет знак с отрицательного на положительный при переходе через точку $ x^* $, то $ x^* $ - локальный минимум.

Эти условия позволяют анализировать свойства точек экстремума в одномерной оптимизации.

---

# 2. Многомерная оптимизация без ограничений
### Разложение в ряд Тейлора

Развитие функции многих переменных в ряд Тейлора

Рассматриваем функцию многих переменных $y(x) = y(x_1, x_2, x_3, . . . , x_n )$, которая представляет собой критерий оптимальности, чей минимум/максимум мы пытаемся определить. Здесь $x$ - вектор переменных состояния, обозначенный и определенный как $x = [x_1, x_2, x_3, . . . , x_n ]^T$.
$$
y(x) = y(x1,x2,x3,...,x_n)\tag{1}  
$$
Предполагая, что функция (1) определена и непрерывна вместе со своими $j + 1$ производными в окрестности точки $x^*$, тогда обычно разложение в ряд Тейлора в его общей форме записывается как:

$$
    y(x) = 
        y(x^*) + 
        \frac{\Delta y}{1!} + 
        \frac{\Delta^2 y}{2!} + 
        . . . +
        \frac{\Delta^j y}{j!} + R_{j+1} 
\tag{2}
$$

где $R_{j+1}$ - остаток разложения в ряд, а остальные члены, вместе с аналогичным обобщением на более высокие производные, представлены следующим образом:

$$ \Delta y = 
        (x_1 - x^*_1) \frac{\partial y}{\partial x_1}\Big|_{x^*} + 
        (x_2 - x^*_2) \frac{\partial y}{\partial x_2}\Big|_{x^*} + 
        \ldots + 
        (x_n - x^*_n) \frac{\partial y}{\partial x_n}\Big|_{x^*}

\newline

    \Delta^2 y = 
        (x_1 - x^*_1)^2 \frac{\partial^2 y}{\partial x_1^2}\Big|_{x^*} + 
        2(x_1 - x^*_1) (x_2 - x^*_2) \frac{\partial^2 y}{\partial x_1 \partial x_2}\Big|_{x^*} + 
        (x_2 - x^*_2)^2\frac{\partial^2 y}{\partial x_2^2}\Big|_{x^*} + 
        \ldots + 
        (x_n - x^*_n)^2\frac{\partial^2 y}{\partial x_n^2}\Big|_{x^*}
\tag{3} $$

(и так далее для более высоких порядков).

В общем виде:

$$
    \Delta^k y = 
        \sum_{p_1, p_2, \ldots, p_n} 
            \frac{k!}{p_1! p_2! \ldots p_n!} 
            (x_1 - x^*_1)^{p_1} (x_2 - x^*_2)^{p_2} \ldots (x_n - x^*_n)^{p_n} 
            \frac{\partial^k y}{\partial x_1^{p_1} \partial x_2^{p_2} \ldots \partial x_n^{p_n}}\Big|_{x^*}
\tag{4}
$$

Здесь $p_1, p_2, ..., p_n$ - неотрицательные целые числа, и суммирование происходит по всем возможным наборам $p_1, p_2, ..., p_n$ таким, что $p_1 + p_2 + ... + p_n = k$.

Тогда для функции двух переменных 
$$ 
    y(\hat{x}) = y(x1, x2) 
\tag{5}
$$
:
$$
    \Delta y = 
        (x_1 - x^*_1)\frac{\partial y}{\partial x_1}\Big|_{x^*} +         
        (x_2 - x^*_2)\frac{\partial y}{\partial x_2}\Big|_{x^*}

\newline

    \Delta^2 y = 
        (x_1 - x^*_1)^2\frac{\partial^2 y}{\partial x_1^2}\Big|_{x^*} + 
        2(x_1 - x^*_1)(x_2 - x^*_2) \frac{\partial^2 y}{\partial x_1 \partial x_2}\Big|_{x^*} + 
        (x_2 - x^*_2)^2\frac{\partial^2 y}{\partial x_2^2}\Big|_{x^*}
\tag{6}
$$


Применяя рассуждения из одномерной оптимизации, мы можем ввести замену $x_i - x^*_i = h_i$ и упростить выражения (6), записав их в более компактной форме, более удобной для дальнейшего анализа:

$$
    \Delta y = 
        h_1 \frac{\partial y}{\partial x_1}\Big|_{x^*} +
        h_2 \frac{\partial y}{\partial x_2}\Big|_{x^*}

\newline

    \Delta^2 y = 
        h_1^2 \frac{\partial^2 y}{\partial x_1^2}\Big|_{x^*} + 
        2h_1 h_2 \frac{\partial^2 y}{\partial x_1 \partial x_2}\Big|_{x^*} + 
        h_2^2 \frac{\partial^2 y}{\partial x_2^2}\Big|_{x^*}
\tag{7}
$$

### Необходимые и достаточные условия экстремума функции двух переменных

Необходимые и достаточные условия для экстремума функции двух переменных напоминают случай функции одной переменной. Напоминаем выражение для разницы функции одной переменной:

$$y(x^* + h) - y(x^*) = h y'(x^*) + \frac{h^2}{2} y''(x^*)$$

Здесь, из-за колебаний знака $h$, мы вводим необходимое условие того, что первая производная в точке $x^*$ равна нулю, то есть $y'(x^*) = 0$. Аналогичное рассуждение применяется и для функции двух переменных.

В случае функции нескольких переменных, сводится к условию, что все частные производные по переменным $x_i$ равны нулю. То есть:

$$ 
\frac{\partial y}{\partial x_i}\Bigg|_{x^*} = 0 \text{ для всех i = 1, 2}  \tag{8}
$$.

Все кандидаты на (локальный) минимум/максимум находятся среди стационарных точек. Это утверждение можно сформулировать следующим интуитивным образом.

> **Утверждение 1. (Необходимые условия экстремума функции двух переменных):** В случае, если существуют частные производные первого порядка функции $y$ по всем переменным, то точка экстремума функции $y$ в точке $x^*$ удовлетворяет системе уравнений $$ \frac{\partial y}{\partial x_i}\Bigg|_{x^*} = 0 \quad \text{ для } i = 1, 2 $$ или условию $$ \Delta y = 0 $$.

Если точка $ (x^*_1, x^*_2) $ удовлетворяет условиям стационарности (необходимые условия экстремума), то она считается стационарной точкой. Тогда, необходимые условия для экстремума могут быть выражены следующим образом:

$$ y(x) - y(x^*) = \frac{\Delta^2 y}{2!} \tag{9}$$

где $ \Delta^2 y $ представляет собой изменение производной функции второго порядка, а $ y(x^*) $ - значение функции в стационарной точке $ (x^*_1, x^*_2) $.

Далее, из предыдущего выражения ясно видно, что стационарная точка $ (x^*_1, x^*_2) $ имеет максимальное значение, если правая часть уравнения меньше нуля, то есть минимальное значение достигается при $ \Delta^2 y < 0 $. Предположим, что нас интересует, когда функция (5) достигает максимального значения, то есть когда выражение $ \Delta^2 y < 0 $:

$$
    \Delta^2 y = 
        h^2_{1} \frac{\partial^2 y}{\partial x_{1}^2} \Bigr|_{\ast} + 
        2h_{1}h_{2} \frac{\partial^2 y}{\partial x_{1} \partial x_{2}} \Bigr|_{\ast} + 
        h^2_{2} \frac{\partial^2 y}{\partial x_{2}^2} \Bigr|_{\ast} < 0 
    \text{ при нетривиальных } h_1 \text{ и } h_2 
\tag{11}
$$


> Если значение функции $ \Delta^2 y > 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \Delta^2 $ положительно определена.

> Если значение функции $ \Delta^2 y \leq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \Delta^2 $ отрицательно полуопределена.

> Если значение функции $ \Delta^2 y \geq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \Delta^2 $ положительно полуопределена.

> Если значение функции $ \Delta^2 y \neq 0 $ для нетривиальных значений $ h_1 $ и $ h_2 $, то мы говорим, что функция $ \Delta^2 $ неопределена.


Если условие (11) выполняется, мы утверждаем, что функция $ \Delta^2 y $ отрицательно определена. Обычно в литературе понятие определенности связано с концепциями минимума и максимума, а процесс выведения достаточных условий называется исследованием определенности. Как вы увидите, это не тривиальная задача и, вероятно, является наиболее сложной в этой части курса.

У нашем конкретном случае, наша цель - вывести достаточные условия, при которых справедливы соотношения (10) и (11). Это сводится к тому, чтобы представить эти уравнения в компактной форме, в которой все члены $h_1$ и $h_2$ будут под знаком квадрата, что гарантирует инвариантность условий относительно знака параметров $h_i$.

В процессе вывода, который следует, мы следуем логике, которая приведет нас к условиям, гарантирующим отрицательную (положительную) определенность. Другие граничные случаи мы рассмотрим через примеры, и они не являются предметом данного вопроса. Выражение (10) мы можем разложить следующим образом:

$$ 
    \Delta^2 y = 
    \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
    \left[
        h^2_{1} +
        2h_1 h_2 \frac{
            \left( \frac{\partial^2 y}{\partial x_{1}\partial x_{2}} \right)^*
        }{
            \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
        } +
        h^2_2 \frac{
            \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^*
        }{
            \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
        }
    \right] < 0
\tag{12}
$$


Первые два члена в скобках можно рассматривать как часть бинома, а остальные члены мы сгруппируем отдельно. В выражении (13) мы достигли того, что все члены $h_i$ теперь под знаком квадрата, и их вклад в характер экстремума теперь не вызывает сомнений. Таким образом, на определенность теперь влияют только члены, для которых мы предполагаем, что они меньше 0.

$$ 
    \Delta^2 y = 
        \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
        \left[
            h^2_{1} +
            2h_1 h_2 \frac{
                \left( \frac{\partial^2 y}{\partial x_{1}\partial x_{2}} \right)^*
            }{
                \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
            }
        \right]^{2} +
        h^2_2 \left[
            \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^* - 
            \frac{
                \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^{*2}
            }{
                \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
            }
        \right] < 0
\tag{13}
$$

В выражении (13) мы добились того, что все члены $h_i$ теперь под знаком квадрата, и их вклад в характер экстремума теперь не вызывает сомнений, то есть на определенность теперь влияют только члены, для которых мы предполагаем, что они меньше 0.

$$
    \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^* < 0 
\text{  и  }
    \left[
        \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^* - 
        \frac{
            \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^{*2}
        }{
            \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
        }
    \right] < 0
\tag{14}
$$

Выражение в средней скобке может быть записано в виде рационального выражения, а именно, как

$$
    \frac{
        \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^*
        \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
        -
        \left( \frac{\partial^2 y}{\partial x_{1}\partial x_{1} \partial x_{2}} \right)^{*2}
    }{
        \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^*
    } < 0
\tag{15}
$$
Перевод на русский:

Первое условие из выражения (14),
$
    \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^* < 0
$
однозначно указывает на то, что знаменатель выражения (15) должен быть меньше нуля, а числитель должен быть больше нуля, следовательно, условия отрицательной определённости, наконец, становятся:
$$
    \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^* < 0 
\tag{16}
$$
и
$$
    \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^*
    \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^* -
    \left( \frac{\partial^2 y}{\partial x_{1}\partial x_{1} \partial x_{2}} \right)^{*2} > 0
\tag{17}
$$

Выражения (16) и (17) представляют собой достаточные условия того, что функция двух переменных имеет максимум, то есть что функция (10) отрицательно определена.

Читателям предоставляется возможность самостоятельно вывести условия положительной определённости, которые будут получены без значительных трудностей в следующей форме:
$$
    \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^* > 0 
\tag{18}
$$
$$
    \left( \frac{\partial^2 y}{\partial x_{2}^2} \right)^*
    \left( \frac{\partial^2 y}{\partial x_{1}^2} \right)^* -
    \left( \frac{\partial^2 y}{\partial x_{1}\partial x_{1} \partial x_{2}} \right)^{*2} > 0
\tag{19}
$$

Перевод на русский:

Тем не менее, важно подчеркнуть, что стационарная точка может быть как максимумом, так и минимумом, когда функция $ \Delta^2 y $ является отрицательно полуопределённой или положительно полуопределённой соответственно. О этих случаях мы упомянем в последующих примерах, а читателей направляем на рекомендованную литературу для получения более подробной информации.

Условия определённости (16)-(19) можно более компактно записать с использованием критерия Лагранжа следующим образом:

> Утверждение 2. (Достаточные условия экстремума функции двух переменных)
> В случае, если существуют частные производные второго порядка функции $ y $ по всем переменным в окрестности точки $ x^* $, и если выполняется условие $\frac{\partial y}{\partial x_i}\big|_{x^*} = 0$, то точка экстремума функции $ y $ в точке $ x^* $ удовлетворяет следующим условиям: $ \newline $
> Для строгого локального минимума: $D_i > 0$ для $i = 1, 2$. $ \newline $
> Для строгого локального максимума: $D_1 < 0$ и $D_2 > 0$, где $ \newline $
> $$ D_1 = \left|\frac{\partial^2 y}{\partial x_1^2}\right|_{x=x^*}, \quad D_2 = \left|\begin{matrix} \newline \frac{\partial^2 y}{\partial x_1^2} & \frac{\partial^2 y}{\partial x_1 \partial x_2} \newline \frac{\partial^2 y}{\partial x_1 \partial x_2} & \frac{\partial^2 y}{\partial x_2^2} \newline \end{matrix}\right|_{x=x^*} $$


Введенные достаточные условия представляют собой очень строгие требования, которые функция должна удовлетворять в отношении экстремума.

### Необходимые и достаточные условия экстремума функции многих переменных

"Потребные и достаточные условия функции многих переменных (*В нашем случае двух переменных*) мы не будем отдельно выводить и доказывать, а логически обобщим изучение из предыдущего параграфа. Рассматриваем функцию, которая в общем случае зависит от n переменных:
$$ y(x) = y(x_1, x_2, x_3, \ldots, x_n) . \tag{20} $$
Как мы показали, разложение в ряд функции многих переменных сводится к выражению (2), при этом наиболее значимыми являются первые два члена с правой стороны равенства, то есть
$$ y(x) - y(x^*) = \Delta y + \frac{\Delta^2 y}{2} . \tag{21} $$
Необходимые условия экстремума получаются обобщением Утверждения 1.2
> Утверждение 3. (Необходимые условия экстремума функции многих переменных)
> В случае наличия частных производных первого порядка функции $y$ по всем переменным, точка экстремума функции $y$ в точке $x^*$ удовлетворяет системе уравнений
> $$ \frac{\partial y}{\partial x_i}\Big|_{x^*} = 0 \text{ для } i = 1, \ldots , n $$
> или условию
> $$ \Delta y = 0 $$

Чтобы описать сложные связи из условий (16)-(19) в случае функции двух переменных, мы ввели матричную форму вторых производных, которая для функции (20) может быть записана следующим образом:
$$
    H = 
    \begin{bmatrix}
        \frac{\partial^2 y}{\partial x_1^2} & \frac{\partial^2 y}{\partial x_1 \partial x_2} & \frac{\partial^2 y}{\partial x_1 \partial x_3} & \ldots & \frac{\partial^2 y}{\partial x_1 \partial x_n} \\
        \frac{\partial^2 y}{\partial x_2 \partial x_1} & \frac{\partial^2 y}{\partial x_2^2} & \frac{\partial^2 y}{\partial x_2 \partial x_3} & \ldots & \frac{\partial^2 y}{\partial x_2 \partial x_n} \\
        \frac{\partial^2 y}{\partial x_3 \partial x_1} & \frac{\partial^2 y}{\partial x_3 \partial x_2} & \frac{\partial^2 y}{\partial x_3^2} & \ldots & \frac{\partial^2 y}{\partial x_3 \partial x_n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial^2 y}{\partial x_n \partial x_1} & \frac{\partial^2 y}{\partial x_n \partial x_2} & \frac{\partial^2 y}{\partial x_n \partial x_3} & \ldots & \frac{\partial^2 y}{\partial x_n^2}
    \end{bmatrix}
\tag{22}
$$

Эта матрица (22) называется Гессианом, и она играет важную роль в наших последующих рассмотрениях.

Как мы видели на примере функции двух переменных, определение закономерностей, когда квадратичная форма (7) положительно/отрицательно определена, не является простым и дополнительно усложняется в случае, когда функция зависит от более чем двух переменных (3). Для проверки определённости, как правило, используется Теорема Сильвестра.

> Утверждение 4. (Сильвестрова теорема, условия определённости) Симметричная квадратная матрица $A = [a_{ij}]$ размерности $n$ является положительно определённой тогда и только тогда, когда все главные миноры (миноры вокруг главной диагонали) матрицы $A$ положительны, то есть $D_i > 0$ для $i = 1, 2, \ldots, n$, и отрицательно определённой тогда и только тогда, когда главные миноры (миноры вокруг главной диагонали) матрицы $A$ чередуют знак следующим образом: $ \newline $
> $\newline \newline D_{1,3,5,7,...} < 0, \quad D_{2,4,6,8,...} > 0 \newline \newline $
> где $ \newline $
> $\newline D_1 = \left|a_{11}\right|, \quad D_2 = \left|\begin{array}{cc} a_{11} & a_{12} \\ a_{12} & a_{22} \end{array}\right|, \quad \ldots, \quad D_n = \left|\begin{array}{cccc} a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\ a_{21} & a_{22} & a_{23} & \ldots & a_{2n} \\ a_{31} & a_{32} & a_{33} & \ldots & a_{3n} \\ \ldots & \ldots & \ldots & \ldots & \ldots \\ a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nn} \end{array}\right| \newline $

Наконец, легко заметить, что квадратная форма функции нескольких переменных, для которой мы исследуем определённость (3), фактически представлена гессианом (22). Исследование определённости сводится к анализу главных миноров матрицы Гессе в соответствии с Тверджением 4.

> Утверждение 5. (Достаточные условия экстремума функции нескольких переменных) В случае наличия частных производных второго порядка функции $y$ по всем переменным в окрестности точки $x^*$ и при условии $\Delta^2 y\big|_{x^*} = 0$, точка экстремума функции $y$ в точке $x^*$ удовлетворяет следующим условиям:
> - Для строгого локального минимума, $D_i > 0$ для $i = 1, 2, \ldots, n$.
> - Для строгого локального максимума, $D_{1,3,5,7,...} < 0$, $D_{2,4,6,8,...} > 0$, где
> $$
> D_1 = \left|\frac{\partial^2 y}{\partial x_1^2}\right|_{x=x^*}, \quad D_2 = \left|\begin{array}{cc} \frac{\partial^2 y}{\partial x_1^2} & \frac{\partial^2 y}{\partial x_1 \partial x_2} \\ \frac{\partial^2 y}{\partial x_1 \partial x_2} & \frac{\partial^2 y}{\partial x_2^2} \end{array}\right|_{x=x^*}, \ldots, \quad D_n = \left|\frac{\partial^2 y}{\partial x_n^2}\right|_{x=x^*},
> $$
> и матрица $H$ имеет форму, представленную в выражении (22).


Хорошо, зададим следующий вопрос: если условие для строгого минимума/максимума является положительной/отрицательной определенностью функции, будет ли под теми же условиями полуопределенность гарантировать минимум/максимум (без строгого префикса)? Ответ - ДА. Условия для проверки полуопределенности в духе Утверждения 5 являются сложными, поэтому мы предоставляем альтернативный метод проверки определенности, который вам хорошо известен из теории систем автоматического управления.

> Утверждение 6. (Собственные значения и характер экстремума) Пусть $\lambda_1, \lambda_2, \ldots, \lambda_n$ - собственные значения матрицы Гессе $H$. Матрица считается:
> - положительно определенной, если все значения $\lambda_i > 0$ для $i = 1, 2, \ldots, n$,
> - положительно полуопределенной, если все значения $\lambda_i \geq 0$ для $i = 1, 2, \ldots, n$,
> - отрицательно определенной, если все значения $\lambda_i < 0$ для $i = 1, 2, \ldots, n$,
> - отрицательно полуопределенной, если все значения $\lambda_i \leq 0$ для $i = 1, 2, \ldots, n$,
> - неопределенной, если собственные значения изменяют знак. 

> Условие $\leq i \geq$ логически подразумевает, что существует хотя бы одно значение, отличное от нуля.

### Линейная регрессия и метод наименьших квадратов

Начнем с процедуры линейной регрессии, опираясь на метод наименьших квадратов (поскольку эти процедуры являются основой машинного обучения) и представим их как задачу оптимизации. С самого начала необходимо определить критерий оптимальности и представить его в квадратичной форме, где цель - минимизировать ошибку:

$$ F = \sum_{k=1}^{n} \left( y(x_k) - y_k \right)^2 \tag{3}$$ 

Поскольку мы решаем проблему линейной регрессии, критерий оптимальности теперь принимает форму:

$$ F = \sum_{k=1}^{n} \left( a_1x_k + a_0 - y_k \right)^2 \tag{4}$$

Как мы ранее отметили, наша цель - определить оптимальные значения параметров $a_0$ и $a_1$, чтобы критерий оптимальности (2) был минимален. На основе необходимых условий экстремума получаем следующую систему уравнений:

$$ \begin{cases}
\frac{\partial F}{\partial a_0} = \sum_{k=1}^{n} 2(a_1x_k + a_0 - y_k) = 0 \\
\frac{\partial F}{\partial a_1} = \sum_{k=1}^{n} 2(a_1x_k + a_0 - y_k)x_k = 0
\end{cases} \tag{5}$$

Линейная система уравнений (5) также может быть записана в матричной форме:

$$ \begin{bmatrix} n & \sum_{k=1}^{n} x_k \\ \sum_{k=1}^{n} x_k & \sum_{k=1}^{n} x_k^2 \end{bmatrix} \begin{bmatrix} a_0 \\ a_1 \end{bmatrix} = \begin{bmatrix} \sum_{k=1}^{n} y_k \\ \sum_{k=1}^{n} x_ky_k \end{bmatrix} \tag{6} $$

Решим ее методом Краммера: 

Mетод Краммера представляет собой теорему в линейной алгебре, которая предоставляет решения системы линейных уравнений с использованием детерминант. Если систему уравнений представить в виде умножения матриц: $Ax = c$, где $A$ - квадратная матрица, $x$ - вектор-столбец переменных, и матрица $A$ является регулярной (невырожденной), то решение можно выразить следующим образом:

$$x_i = \frac{\text{det}(A_i)}{\text{det}(A)}$$

где $A_i$ - матрица, полученная заменой $i$-го столбца в матрице $A$ вектором-столбцом $c$.

В дальнейшем мы рассмотрим решение матричного уравнения (8) и его дальнейшее применение в изучении проблемы регрессии. Решение системы (6) получается с использованием правила Крамера. После решения системы уравнений и получения оптимальных значений параметров $a_0$ и $a_1$ формально завершается процесс линейной регрессии.

$$ a_1 = \frac{n \sum_{k=1}^{n} x_k y_k - \sum_{k=1}^{n} x_k \sum_{k=1}^{n} y_k}{n \sum_{k=1}^{n} x_k^2 - (\sum_{k=1}^{n} x_k)^2} \tag{7a}$$

$$ a_0 = \frac{\sum_{k=1}^{n} y_k - a_1 \sum_{k=1}^{n} x_k}{n} \tag{7b}$$

Где $a_0^*$ и $a_1^*$ обозначают оптимальные значения параметров $a_0$ и $a_1$ в соответствии с предполагаемым критерием оптимальности (2). Оптимальное значение критерия обозначим как $F_{LSE}$ и вычислим его следующим образом:

$$ F_{LSE} = \sum_{i=1}^{n} (a_1^*x_i + a_0^* - y_i)^2 \tag{8} $$

---


# 3. Оптимизация с ограничениями типа равенства, методы замены и ограниченные вариации
### **Введение**

В предыдущей главе мы занимались задачей свободной оптимизации, где переменные состояния не были подвергнуты каким-либо ограничениям. В этой главе рассмотрим более реалистичную задачу, где введены ограничения на переменные состояния, причем эти ограничения выражены в виде уравнений. Эти ограничения, как вы увидите далее, обычно являются следствием физических и логических закономерностей, и их наличие существенно влияет на исследование оптимизационной задачи.

Задачу оптимизации с ограничениями типа уравнений можно сформулировать следующим образом, чтобы найти экстремум функции

$$y = y(x_1, x_2, x_3, \ldots, x_n), \tag{1}$$

при условии, что переменные состояния $x_i$ должны удовлетворять следующей системе уравнений. Этот набор ограничений мы будем далее записывать как

$$g_k(x) = 0 \text{ или } g_k(x_i) = 0 \text{ для } k = 1, \ldots, m \text{ и } i = 1, \ldots, n. \tag{2}$$

где условие $m < n$ должно выполняться, то есть количество ограничений $g_k$ строго меньше количества переменных состояния $x_i$. Действительно, если это условие не выполняется, решая систему уравнений (2), мы могли бы определить все переменные состояния, и тогда у нас не останется ни одной степени свободы, ни одной $x_i$, изменением которой мы могли бы достичь лучшего или оптимального решения.

В дальнейшем мы рассмотрим четыре метода оптимизации, характерных для этого класса задач: метод подстановки, метод ограниченной вариации, метод множителей Лагранжа.

### **Метод замены**
Первый шаг в применении этого метода - решение системы уравнений ограничений (2) относительно $m$ переменных. Полученные решения затем подставляются в критерий оптимальности (1), что уменьшает размерность до $n - m$ переменных, и критерий оптимальности теперь принимает вид:

$$y = y(x_1, x_2, x_3, \ldots, x_{n-m}) \tag{3}$$

Дальнейшее решение происходит по принципам свободной оптимизации, описанным в предыдущем разделе. На первый взгляд этот процесс кажется очень простым, хотя мы интуитивно понимаем, что решение системы ограничений (2) может быть затруднено. В продолжении текста мы иллюстрируем некоторые преимущества и недостатки этого метода через примеры.
Пример 1. Метод подстановки, линейные ограничения.

Рассмотрим критерий оптимальности с двумя переменными состояниями:

$$y(x_1, x_2) = 4x_1^2 + 5x_2^2.$$

Переменные состояния $x_1$ и $x_2$ должны удовлетворять ограничению:

$$2x_1 + 3x_2 = 6.$$

Следуя предложенной формализации метода подстановки, первый шаг - решение ограничения относительно одной переменной, например, $x_1$:

$$x_1 = \frac{6 - 3x_2}{2}.$$

Проекция функции $y(x_1, x_2) = 4x_1^2 + 5x_2^2$ на плоскость $x_1, x_2$ изображена на рисунке 1.

Заменяя это полученное решение в критерий оптимальности, получаем:

$$y = 4\left(\frac{6 - 3x_2}{2}\right)^2 + 5x_2^2 = 14x_2^2 - 36x_2 + 36.$$

Дальнейшее решение просто, сводится к поиску первой и второй производной для необходимых и достаточных условий соответственно, по оставшейся переменной $x_2$:

$$\frac{\partial y}{\partial x_2} = 28x_2 - 36 = 0 \Rightarrow x_2^* = \frac{36}{28} \approx 1.286.$$

$$\frac{\partial^2 y}{\partial x_2^2} = 28 > 0, \text{ точка - минимум}.$$

### **Метод ограниченных вариаций**

**Вводные замечания**

В предыдущем разделе мы рассмотрели проблему свободной оптимизации, где переменные состояния не были подвергнуты каким-либо ограничениям. В этом разделе мы будем рассматривать более реалистичную проблему, где введены ограничения на переменные состояния в виде уравнений $g(x_1, x_2) = 0$. Эти ограничения, как правило, обусловлены физическими и логическими закономерностями и существенно влияют на изучение оптимизационной задачи.

**Формулировка задачи**

Задачу оптимизации с ограничениями типа равенства можно сформулировать следующим образом, чтобы найти экстремум функции

$$y = y(x_1, x_2, \ldots, x_n), \quad (1)$$

при условии, что переменные состояния $x_i$ должны удовлетворять следующей системе уравнений:

$$g_k(x) = 0 \quad \text{или} \quad g_k(x_i) = 0, \quad \text{для} \quad k = 1, \ldots, m \quad \text{и} \quad i = 1, \ldots, n. \quad (2)$$

С условием $m < n$, что означает, что количество уравнений $g_k$ строго меньше количества переменных состояния $x_i$.

**Необходимые условия экстремума**

Если точка $(x^*_1, x^*_2)$ является экстремумом функции, то в этой точке выполняются необходимые условия экстремума:

$$dy = 0 = \frac{\partial y}{\partial x_1}dx_1 + \frac{\partial y}{\partial x_2}dx_2, \quad (6)$$

где $dy$ - полный дифференциал функции, и это соотношение справедливо только для допустимых приращений $dx_1$ и $dx_2$ (см. Рисунок 2). В точке экстремума также должно выполняться $g(x^*_1, x^*_2) = 0$. Допустимые приращения должны обеспечивать, чтобы все движения из точки экстремума $(x^*_1 + dx_1, x^*_2 + dx_2)$ также удовлетворяли ограничению $g(x^*_1 + dx_1, x^*_2 + dx_2) = 0$.

Разложением уравнения (7) с учетом малых значений $dx_1$ и $dx_2$ получаем

$$g(x^*_1, x^*_2) + \frac{\partial g}{\partial x_1}dx_1 + \frac{\partial g}{\partial x_2}dx_2 = 0, \quad (8)$$

при условии, что $\frac{\partial g}{\partial x_1} \neq 0$, мы можем выразить допустимое приращение $dx_2$ из этого уравнения:

$$dx_1 = -\frac{\partial g}{\partial x_2}\frac{\partial g}{\partial x_1}dx_2. \quad (9)$$

Подставив это выражение в (6), получим

$$dy = 0 = \frac{\partial y}{\partial x_2} - \frac{\partial g}{\partial x_2}\frac{\partial g}{\partial x_1}\frac{\partial y}{\partial x_1}dx_2. \quad (10)$$

Для произвольного значения $dx_1$ равенство (10) будет выполнено только при условии, что выражение в скобках равно нулю или при выполнении условия

$$\frac{\partial y}{\partial x_2}\frac{\partial g}{\partial x_1} - \frac{\partial g}{\partial x_2}\frac{\partial y}{\partial

 x_1} = 0. \quad (11)$$

Это условие (11) представляет собой необходимые условия локального экстремума функции двух переменных. При вычислении конкретных точек необходимо учесть также уравнение ограничения (5). Отметим, что рассмотрение достаточных условий не представляет интереса в нашем исследовании. Уравнение (11) может быть записано в матричной форме в виде якобиана:

$$\begin{vmatrix}\frac{\partial y}{\partial x_2} & \frac{\partial y}{\partial x_1}\\ \frac{\partial g}{\partial x_2} & \frac{\partial g}{\partial x_1}\end{vmatrix} \neq 0. \quad (12)$$

где условие $\frac{\partial g}{\partial x_1} \neq 0$ записывается как $J(g_{x_1}) \neq 0$. В более общем случае, когда критерий оптимальности зависит от $n$ переменных и имеется $m$ ограничений, в точке экстремума выполнено:

$$J(y, g_{x_1}, \ldots, g_{x_m}) \neq 0.$$

**Общая формулировка условий**

Рассмотрим функцию $y = y(x_1, x_2, x_3, \ldots, x_n)$ с ограничениями $g_k(x_1, x_2, x_3, \ldots, x_n) = 0$ для $k = 1, \ldots, m$ (13). 

$$ 
y = y(x_1, x_2, x_3, \ldots, x_n) 
\newline 
g_k(x_1, x_2, x_3, \ldots, x_n) = 0 \text{  для  } k = 1, \ldots, m (13)
$$

Мы можем обобщить выражения (6) и (8) следующим образом:

$$dy = \sum_{i=1}^{n} \frac{\partial y}{\partial x_i}dxi = 0 \quad (14)$$

$$dg_k = \sum_{i=1}^{n} \frac{\partial g_k}{\partial x_i}dxi = 0 \quad (15)$$

Здесь, как и ранее, $dxi$ обозначают допустимые приращения, которые обеспечивают выполнение ограничения $g_k(x_i) = 0$. Аналогично случаю с двумя переменными, из m уравнений (15) мы можем вычислить столько же допустимых приращений в следующей процедуре, и уравнение (16) может быть записано в виде:

$$\sum_{i=1}^{m} \frac{\partial g_k}{\partial x_i}dxi = -\sum_{j=m+1}^{n} \frac{\partial g_k}{\partial x_j}dxj \quad \text{для} \quad k = 1, \ldots, m \quad (16)$$

Также, как и в случае двух переменных, если уравнения (16) линейно независимы, то мы можем решить систему из m допустимых приращений для оставшихся n−m переменных. Подставив полученное решение в (14), получим:

$$dy = \frac{1}{J(g_{x_1}, \ldots, g_{x_m})} \sum_{k=m+1}^{n} J_k \left| \frac{\partial y}{\partial x_k} \frac{\partial g_1}{\partial x_1} \ldots \frac{\partial g_m}{\partial x_m} \right| dx_k = 0 \quad (17)$$

где

$$J_k = \left| \begin{matrix} \frac{\partial y}{\partial x_k} & \frac{\partial y}{\partial x_1} & \frac{\partial y}{\partial x_2} & \ldots & \frac{\partial y}{\partial x_m} \\ \frac{\partial g_1}{\partial x_k} & \frac{\partial g_1}{\partial x_1} & \frac{\partial g_1}{\partial x_2} & \ldots & \frac{\partial g_1}{\partial x_m} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{\partial g_m}{\partial x_k} & \frac{\partial g_m}{\partial x_1} & \frac{\partial g_m}{\partial x_2} & \ldots & \frac{\partial g_m}{\partial x_m} \end{matrix} \right| \quad (18)$$

Если рассмотреть выражение под суммой в (17), то станет ясно, что для произвольных значений допустимых приращений равенство может быть удовлетворено только тогда, когда

$$J_k = \left| \frac{\partial y}{\partial x_k} \frac{\partial g_1}{\partial x_1} \ldots \frac{\partial g_m}{\partial x_m} \right| = 0 \quad \text{для} \quad k = m + 1, m + 2, \ldots, n \quad (19)$$

Уравнение (19) вместе с ограничениями $g_k(x_1, x_2, x_3, \ldots, x_n) = 0$ (20) представляют собой необходимые условия локального экстремума функции в точке экстремума.

---

# 4. Оптимизация с ограничениями типа равенства, метод множителей Лагранжа

### **Метод множителей Лагранжа**

Напоминаем, что оптимизационная задача, которую мы решаем, формулируется следующим образом: найти экстремум функции

$$y = y(x_1, x_2, \ldots, x_n),$$

при условии, что переменные связаны следующим образом:

$$g_k(x_1, x_2, \ldots, x_n) = 0, \text{ для } k = 1, 2, \ldots, m,$$

при условии, что количество ограничений строго меньше количества переменных, $m < n$. 

Для оптимизации с ограничениями типа равенства, метод множителей Лагранжа и метод штрафных функций являются широко используемыми подходами.

Объяснение метода множителей Лагранжа начнем с формализации алгоритма. Сначала мы представим расширенный критерий оптимальности $L$, называемый Лагранжианом:

$$L \equiv F = y(x_1, x_2, \ldots, x_n) + \sum_{k=1}^{m} \lambda_k g_k(x_1, x_2, \ldots, x_n),$$

где $\lambda_k$ обозначают введенные вспомогательные переменные, называемые множителями Лагранжа, объединяющими критерий оптимальности (1) и ограничения (2) в новый расширенный критерий оптимальности. Основная сила метода множителей Лагранжа заключается в том, что он обрабатывает все переменные одинаково.

1. **Составление расширенного критерия оптимальности $L$:**

$$L = y(x_1, x_2, \ldots, x_n) + \sum_{k=1}^{m} \lambda_k g_k(x_1, x_2, \ldots, x_n)$$

2. **Условия первого порядка:**

Частные производные по всем оригинальным переменным равны нулю:

$$\frac{\partial L}{\partial x_i} = \frac{\partial y(x_i)}{\partial x_i} + \sum_{k=1}^{m} \lambda_k \frac{\partial g_k(x_i)}{\partial x_i} = 0, \text{ для } i = 1, 2, \ldots, n$$

3. **Условия по множителям Лагранжа:**

Частные производные по множителям Лагранжа равны нулю:

$$\frac{\partial L}{\partial \lambda_k} = g_k(x_i) = 0, \text{ для } k = 1, 2, \ldots, m \text{ и } i = 1, 2, \ldots, n$$

4. **Получение условий экстремума:**

Решение системы уравнений $g_k(x_i) = 0$ и условий по множителям Лагранжа.

Данный алгоритм позволяет определить условия, при которых функция достигает локального экстремума с учетом заданных ограничений.

---

# 5. Оптимизация с ограничениями типа неравенства, введение дополнительной переменной методом замены, ограниченные вариации, множители Лагранжа

## **Вводные рассмотрения**

В этой главе мы будем рассматривать более общий случай оптимизации функции нескольких переменных, когда ограничения задаются в виде неравенств. Нашей целью является демонстрация оптимального значения функции

$$y = y(x_1, x_2, \ldots, x_n),$$

при условии, что переменные состояния $x_i$ ограничены между собой набором неравенств. Во всех наших дальнейших рассмотрениях, за исключением вводного примера, все ограничения должны быть в форме $\leq 0$. Важно отметить, что все алгоритмы для решения задачи неравенств строго требуют такого отношения.

Как вы увидите, все алгоритмы решения проблем с ограничениями типа неравенств строго требуют такого отношения в своих выражениях.

$$g_k(x_1, x_2, \ldots, x_n) \leq 0, \text{ для } k = 1, 2, \ldots, m.$$

В этом случае отношение между числом ограничений $m$ и числом переменных состояния $n$ абсолютно произвольно, то есть не существует математического или логического требования, которое предписывает $m < n$, как в случае ограничений типа равенства.

Важно подчеркнуть, что при решении оптимизационной задачи существует всего два возможных сценария: во-первых, решение такое же, как если бы ограничения отсутствовали, тогда строго выполняются условия неравенства $g_k < 0$ или решение находится на границе, по крайней мере, одного $g_k = 0$. Продолжение будет представлено на примере.

## **Введение дополнительной переменной, приведение неравенств к равенству**

Основная идея этого метода заключается в том, чтобы преобразовать ограничения типа неравенств в ограничения типа равенств, после чего решить задачу с использованием известных методов из предыдущих глав. Ценой этого подхода является увеличение размерности исходной проблемы, что действительно может стать проблемой при большом количестве ограничений.

Начнем с ограничений типа неравенств в общей форме:

$$g_k(x_1, x_2, \ldots, x_n) \leq 0, \text{ для } k = 1, 2, \ldots, m.$$

Чтобы преобразить ограничение (3) в равенство, мы должны ввести неотрицательную функцию $S_k$ следующим образом:

$$\Phi_k = g_k(x_1, x_2, \ldots, x_n) + S_k = 0, \text{ для } k = 1, 2, \ldots, m.$$

Очевидно, что дополнительная функция обладает следующими свойствами: для решения на границе $S_k = 0$, а для решения в допустимой области $S_k > 0$. Чтобы эта дополнительная функция была допустимой и не меняла характер оптимизационной проблемы, функция $S_k$ должна зависеть от введенных новых переменных, количество которых соответствует количеству ограничений. Эти введенные переменные существенно не влияют на изучение нашей оптимизационной проблемы и часто называются паразитарными переменными или переменными слака (slack), потому что они точно не имеют значения, как оригинальные переменные.

$$\Phi_k = g_k(x_1, x_2, \ldots, x_n) + x_{n+k}^2 = 0, \text{ для } k = 1, 2, \ldots, m.$$

Здесь логично ввести дополнительные переменные в ограничения в виде квадратных функций, чтобы обеспечить, что $S_k$ являются неотрицательными дифференцируемыми функциями.

$$ \Phi_k = g_k(x_1, x_2, \ldots, x_n) + x_{n+k}^2 = 0, \text{ для } k = 1, 2, \ldots, m.$$

Очевидно, что в предельном случае $g_k = 0$ подразумевает $x_{n+k} = 0$, а решение в пределах допустимой области $g_k < 0$ означает, что $x_{n+k} \neq 0$.

Как мы уже отметили в начале, очевидно, что этот подход увеличивает размерность исходной проблемы, но решение сводится к хорошо известным методам оптимизации с ограничениями типа равенства. В дальнейшем мы будем иллюстрировать применение метода подстановки и ограниченных вариаций исключительно на примерах, а методу множителей Лагранжа уделим внимание исключительно теоретическим рассмотрениям.

## **Ограничения типа неравенства, метод множителей Лагранжа**

Как и в предыдущем разделе, метод множителей Лагранжа  представлен пошагово для пояснения реализации этого метода.

**1. Ограничения типа неравенства (2)** вводим дополнительные переменные под знаком квадрата $x_{n+k}$ и переводим их в ограничения типа равенства:

$$\Phi_k = g_k(x_1, x_2, \ldots, x_n) + x_{n+k}^2 = 0, \text{ для } k = 1, 2, \ldots, m. (\text{1})$$

   Это увеличивает размерность проблемы до $n + m$, а при наличии множителей Лагранжа до $n + 2m$, что, конечно, увеличивает количество уравнений, которые мы решаем.

**2. Формирование расширенного критерия оптимальности F в форме Лагранжиана**:

$$F = y(x_1, x_2, \ldots, x_n) + \sum_{k=1}^{m} \lambda_k \Phi_k. (\text{2})$$

   Далее решение происходит в духе оптимизации без ограничений, то есть мы ищем, чтобы первая производная по всем переменным была равна нулю, что представляет собой второй, третий и четвертый шаг этой процедуры.

**3. Частные производные по всем исходным переменным равны нулю**:

$$\frac{\partial F}{\partial x_i} = \frac{\partial y(x_i)}{\partial x_i} + \sum_{k=1}^{m} \lambda_k \frac{\partial g_k(x_i)}{\partial x_i} = 0, \text{ для } i = 1, 2, \ldots, n. (\text{3})$$

   Из системы уравнений (3) мы можем рассчитать значения n переменных, но так как цена обобщения критерия оптимальности увеличивается на $n + 2m$, недостающие $2m$ уравнения определяются из условий, что частные производные по паразитическим переменным $x_{n+k}$ и множителям Лагранжа равны нулю, что полностью соответствует принципам этой теории, которая рассматривает все переменные одинаково, т.е.

**4. Частные производные по всем новым переменным $x_{n+k}$ равны нулю**:

$$\frac{\partial F}{\partial x_{n+k}} = 2\lambda_k x_{n+k} = 0, \text{ для } k = 1, 2, \ldots, m. (\text{4})$$

**5. Частные производные по множителям Лагранжа равны нулю**:

$$\frac{\partial F}{\partial \lambda_k} = g_k(x_i) + x_{n+k}^2 = 0, \text{ для } k = 1, 2, \ldots, m, \text{ для } i = 1, 2, \ldots, n. (\text{5})$$

   Или уравнения ограничений (1).

**6. Необходимые условия экстремума получаются решением системы уравнений (3) - (5)**, с проверкой того, удовлетворяют ли все полученные решения ограничениям (2).

**7. Достаточные условия проверяются так же, как и в случае ограничений типа равенства**, учитывая увеличенную размерность из-за присутствия дополнительных переменных $x_{n+k}$.

---

## 6. Условия Каруша-Куна-Таккера (KKT)

Условия Каруша-Куна-Таккера являются набором необходимых условий для оптимальности в задачах математического программирования с ограничениями типа неравенства и равенства. Представленные ниже условия широко используются для анализа оптимальности в задачах выпуклой оптимизации.

1. **Необходимые условия стационарности:**
   - Для точки оптимума $ \mathbf{x}^* $ должно выполняться условие стационарности:
     $$ \nabla f(\mathbf{x}^*) + \sum \lambda_i \nabla g_i(\mathbf{x}^*) + \sum \mu_j \nabla h_j(\mathbf{x}^*) = 0 $$
     где $ \lambda_i $ и $ \mu_j $ - множители Лагранжа для ограничений типа неравенства и равенства соответственно.

2. **Условия дополняющей нежесткости:**
   - Для всех множителей Лагранжа $ \lambda_i $ и $ \mu_j $:
     $$ \lambda_i g_i(\mathbf{x}^*) = 0 $$
     $$ \lambda_i \geq 0 $$
     $$ g_i(\mathbf{x}^*) \leq 0 $$
     $$ h_j(\mathbf{x}^*) = 0 $$

3. **Условия исключения:**
   - Если ограничение $ g_i(\mathbf{x}^*) $ активно (то есть $ g_i(\mathbf{x}^*) = 0 $), то соответствующий множитель Лагранжа $ \lambda_i $ должен быть неотрицателен: $ \lambda_i \geq 0 $.

4. **Дополнительные условия:**
   - Для каждого ограничения типа неравенства $ g_i(\mathbf{x}) \leq 0 $, и множителя Лагранжа $ \lambda_i $:
     $$ \lambda_i g_i(\mathbf{x}^*) = 0 $$

5. **Слабая устойчивость:**
   - Матрица Якоби ограничений типа неравенства в активных точках оптимума должна быть полноранговой.

Условия Каруша-Куна-Таккера предоставляют важные инсайты для понимания оптимальности в задачах с ограничениями и используются при разработке алгоритмов оптимизации и анализе решений.

---

# 7. Линейное программирование, графический метод, принципы Simplex метода, Simplex метод

### TODO

---

# 8. Численные методы для одномерных задач, градиентные методы, метод Ньютона-Рафсона, Метод Секущих

**Основная идея этих методов:**
Нахождение стационарных точек функции $(f'(x) = 0)$, при условии, что функция дифференцируема до необходимого для нас порядка.

## **Метод Ньютона-Рапсона**
**Метод Ньютона-Рапсона** — это численный метод решения уравнений, который использует итерационный процесс для приближенного нахождения корня уравнения. Алгоритм метода следующий:

1. **Начальный выбор:**
   - Выбирается начальное приближение $x_0$ к корню уравнения.

2. **Итерационный процесс:**
   - На каждом шаге $n$ вычисляется следующее приближение $x_{n+1}$ по формуле:

$$
x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}
$$

   где:
   - $f'(x_n)$ — значение первой производной функции в точке $x_n$,
   - $f''(x_n)$ — значение первой производной функции в точке $x_n$.

3. **Обновление:**
   - Процесс повторяется, и каждый раз вычисляется новое приближение корня.

4. **Проверка сходимости:**
   - Итерации продолжаются до тех пор, пока не будет достигнута заданная точность (критерий сходимости) или выполнено другое условие останова.

Метод Ньютона-Рапсона имеет квадратичную скорость сходимости, что означает, что с каждой итерацией удваивается количество верных знаков в приближении к корню. Однако, он требует наличия производной функции, что может быть вызовом для сложных функций или в случае, когда вычисление производной затруднительно.

## **Метод секущих**
**Метод секущих (метод хорд)** — это численный метод решения уравнений, аппроксимирующий корень путем построения хорды (отрезка) на графике функции и использования ее для приближенного определения корня. 

Основное различие между методом секущих и методом Ньютона-Рафсона заключается в том, что в методе секущих вводится аппроксимация для второй производной:
$$ f''(x_n) = \frac{f'(x_n) - f'(x_{n-1})}{x_n - x_{n-1}} $$

поэтому итерационная формула метода секущих принимает следующий вид:
$$ x_{n+1} = x_n - f'(x_n)\frac{x_n - x_{n-1}}{f'(x_n)-f'(x_{n-1})} $$

Как видно из формулы, теперь не требуется, чтобы функция была дифференцируема до второго порядка, но теперь нам нужны две начальные точки. Выбором хорошего начального интервала (начальных точек) мы позволяем алгоритму сходиться к оптимуму. Этот подход может уменьшить скорость сходимости по сравнению с методом Ньютона-Рафсона.

Алгоритм метода следующий:

1. **Начальный выбор:**
   - Выбираются две начальные точки $x_0$ и $x_1$, близкие к корню уравнения.

2. **Итерационный процесс:**
   - Строится хорда между точками $(x_n, f(x_n))$ и $(x_{n+1}, f(x_{n+1}))$.
   - Новая аппроксимация корня $x_2$ находится как пересечение хорды с осью $x$.

$$ x_{n+1} = x_n - f'(x_n)\frac{x_n - x_{n-1}}{f'(x_n)-f'(x_{n-1})} $$

3. **Обновление:**
   - Точки $x_0$ и $x_1$ обновляются: $x_0 = x_1$, $x_1 = x_2$.

4. **Повторение:**
   - Процесс повторяется до сходимости (достаточного приближения к корню) или до выполнения другого критерия останова.

Метод секущих аппроксимирует производную функции при помощи разностного приближения, и, в отличие от метода Ньютона, не требует вычисления производной в явной форме. Однако, он может быть менее быстрым в сходимости по сравнению с методом Ньютона.

---

## 9. Численные методы для одномерных задач, методы прямого поиска, метод Фибоначчи, метод золотого сечения

**Методы прямого поиска в основном являются методами одномерной оптимизации.**
Считаются "скелетом" для нелинейных алгоритмов оптимизации. Суть заключается в поиске на замкнутом интервале. Часто предполагаем, что функция унимодальна.

**Унимодальная функция** - это функция, которая имеет только один максимум или минимум. Формально, функция $f(x)$ считается унимодальной на интервале, если существует такая точка $x^*$, что:

1. Если $a < x^* < b$, то для всех $x$ в интервале $(a, x^*)$ функция $f(x)$ строго возрастает, и для всех $x$ в интервале $(x^*, b)$ функция $f(x)$ строго убывает.

2. Если $a < b < x^*$, то для всех $x$ в интервале $(a, b)$ функция $f(x)$ строго возрастает.

3. Если $x^* < a < b$, то для всех $x$ в интервале $(x^*, b)$ функция $f(x)$ строго убывает.

Таким образом, унимодальная функция имеет только один пик (максимум или минимум) на заданном интервале. Важным свойством унимодальных функций является возможность применения эффективных методов одномерной оптимизации для их анализа и поиска экстремумов.

## **Метод Фибоначчи**
**Метод Фибоначчи** — это численный метод оптимизации, который используется для нахождения минимума (или максимума) функции в заданном интервале. Алгоритм метода включает следующие шаги:

1. **Выбор начальных точек:**
   - Задаются начальные границы интервала $[a, b]$ и точность $\varepsilon$.

2. **Инициализация ряда Фибоначчи:**
   - Находится такое число $n$, что $F_n \geq \frac{b - a}{\varepsilon}$, где $F_n$ — $n$-е число Фибоначчи.
   - Выбираются начальные точки $x_1$ и $x_2$ внутри интервала так, чтобы длина интервала была равна $\frac{b - a}{F_n}$.

3. **Итерационный процесс:**
   - На каждом шаге процесса точки $x_1$ и $x_2$ обновляются в зависимости от того, какая из подинтервалов $[a, x_2]$ или $[x_1, b]$ содержит минимум (максимум) функции.
   - Длина выбранного подинтервала уменьшается в соответствии с последовательностью чисел Фибоначчи: $F_n$ умножается на $\frac{F_{n-2}}{F_{n-1}}$.
   - Процесс повторяется до достижения заданной точности.

4. **Завершение:**
   - Процесс завершается, когда длина текущего интервала становится меньше или равной $\varepsilon$.

Метод Фибоначчи основан на идее последовательного уточнения интервала, содержащего оптимальное решение, и может быть использован для оптимизации унимодальных функций. Однако, в сравнении с некоторыми другими методами оптимизации, метод Фибоначчи может требовать больше вычислительных ресурсов.

## **Метод Золотого сечения**
**Метод Золотого сечения** — это численный метод оптимизации, используемый для нахождения локального минимума (или максимума) унимодальной функции в заданном интервале. Алгоритм метода включает следующие шаги:

1. **Выбор начальных точек:**
   - Задаются начальные границы интервала $[a, b]$ и точность $\varepsilon$.

2. **Инициализация точек:**
   - Выбираются две промежуточные точки $x_1$ и $x_2$ внутри интервала так, чтобы отношение длин отрезков $[a, x_2]$ и $[x_1, b]$ было равно золотому сечению, т.е.,

$$
\frac{b - x_2}{x_2 - a} = \frac{x_2 - a}{b - x_1} = \phi
$$

где $\phi$ — золотое сечение, приближенно равное 1.618.

3. **Итерационный процесс:**
   - Оценивается значение функции в точках $f(x_1)$ и $f(x_2)$.
   - Сравниваются значения функции в точках $x_1$ и $x_2$, и один из концов интервала сужается в сторону, где функция принимает меньшее значение.
   - Процесс повторяется, пока длина текущего интервала становится меньше или равной $\varepsilon$.

4. **Завершение:**
   - Процесс завершается, когда достигается заданная точность.

Метод Золотого сечения обладает свойством сходиться к оптимальному решению экспоненциально быстро. Этот метод особенно эффективен для унимодальных функций, т.е., функций, имеющих только один локальный экстремум.

---

# 10. Численные методы для многомерных задач, градиентные методы: метод наискорейшего спуска, метод наискорейшего спуска с фиксированным шагом
## **Метод наискорейшего спуска**

Этот метод используется для поиска минимума дифференцируемой функции 
$ f(x) = f(x_1, x_2, \ldots, x_n) $, смещая текущее решение в направлении отрицательного градиента 
$ \nabla f = [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}]^T $ на каждой итерации.

Метод состоит из следующих шагов:

**Инициализация:** Выбираются начальное приближение $ x_0 $, размер шага $ \gamma > 0 $, допустимая погрешность $ \varepsilon > 0 $, и максимальное количество итераций $ N $.

**Тело метода (итеративное применение):** На $ k $-ой итерации у нас есть
$ x_{k+1} = x_k - \gamma \nabla f(x_k) $

**Критерий остановки:** По окончании каждой итерации проверяем условие $ \|\nabla f(x_k)\| \leq \varepsilon $. Когда это условие выполняется или когда мы достигаем максимального допустимого числа итераций, мы прекращаем выполнение алгоритма.

$\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.

## Метод наискорейшего спуска с фиксированным шагом

### TODO

---

# 11. Численные методы для многомерных задач, *градиентные методы*: метод наискорейшего спуска с моментом, ускоренный градиент Нестерова

## **Градиентный метод с моментом**

В методе градиентного спуска с моментом, общая структура алгоритма остается неизменной, но текущая позиция в процессе поиска обновляется немного измененным способом:

$$\mathbf{v}_k = \omega \mathbf{v}_{k-1} + \gamma \nabla f(\mathbf{x}_k)$$

$$\mathbf{x}_{k+1} = \mathbf{x}_{k} - \mathbf{v}_k$$

$\gamma$ (шаг обучения): Значение gamma обычно выбирается в диапазоне от 0.1 до 0.9. Если ваш шаг обучения слишком большой, алгоритм может не сойтись, и, наоборот, если слишком мал, сходимость может быть медленной. Попробуйте начать с относительно небольшого значения, например, 0.1, и увеличивайте его, пока алгоритм не начнет сходиться.

$\omega$ (коэффициент момента): Значение omega также обычно находится в диапазоне от 0.1 до 0.9. Можно начать с маленького значения, например, 0.1, и постепенно увеличивать, чтобы увидеть, как это влияет на производительность. Момент обычно помогает устранить осцилляции и ускорить сходимость.


## **Ускоренный Градиент Нестерова**

Основная идея ускоренного градиента Нестерова заключается в том, что градиент вычисляется в будущей точке.

$$\mathbf{x}'_k = \mathbf{x}_{k-1} - \omega\mathbf{v}_{k-1}$$
$$\mathbf{v}_k = \omega \mathbf{v}_{k-1} + \gamma \nabla f(\mathbf{x}'_k)$$ 
$$\mathbf{x}_{k+1} = \mathbf{x}_{k} - \mathbf{v}_k$$
Ключевым моментом в ускоренном градиенте Нестерова является то, что градиент вычисляется не в текущей точке, а в предполагаемой будущей точке. Таким образом, мы придаем всей процедуре определенный предсказательный характер, ожидая улучшения ее общего поведения.

---

# 12. Численные методы для многомерных задач, адаптивные градиентные методы: ADAGRAD, RMSProp, ADAM

## **АДАГРАД**

Adagrad использует адаптивный градиент, специфичный для каждой оси (каждой переменной).

Пусть $ g_{k, i} $ - градиент критерия оптимальности по $ i $-й переменной в $ k $-й итерации,

### $ G_{k, i} = \sum_{j=1}^{k} (g_{j, i})^2 $

где $ G_{k, i} $ - сумма квадратов градиентов по $ i $-й переменной до $ k $-й итерации.

Обновление $ i $-й переменной:

### $ x_{k+1, i} = x_{k, i} - \frac{\gamma}{\sqrt{G_{k, i} + \epsilon}} g_{k, i} $

где $ \gamma $ - скорость обучения, $ \epsilon $ - малая константа, предотвращающая деление на ноль.

Этот метод позволяет каждой переменной адаптивно регулировать скорость обучения, учитывая её историю градиентов.

## **RMSProp**

Алгоритм RMSProp работает аналогично ADAGRAD, за исключением того, что квадраты градиента не накапливаются бесконечно. Вместо этого вводится процедура, которая поверхностно напоминает процедуру введения момента в градиентном алгоритме.

### $ G_{k+1, i} = \omega G_{k, i} + (1 - \omega) g_{k, i}^2  $

Типичное значение параметра $ \omega $ - 0.9.

Предположим, что $ g^2 $ постоянно. Когда выражение выше сходится, значение $ G $ в установившемся состоянии будет
### $ G = \omega G + (1 - \omega) g^2 $
Иными словами, $ G = g^2 $


## **ADAM**

ADAM (ADAPTIVE MOMENT ESTIMATION) - одна из наиболее широко используемых современных модификаций алгоритма наискорейшего спуска.

Сначала определяются вспомогательные величины:

### $ m_k = \omega_1 m_{k-1} + (1 - \omega_1) g_k $
### $ v_k = \omega_2 v_{k-1} + (1 - \omega_2) g_{k}^2 $

и их скорректированные версии:

### $ \hat{m}_k = \frac{m_k}{1 - \omega_1^k} $
### $ \hat{v}_k = \frac{v_k}{1 - \omega_2^k} $

Затем текущее решение обновляется по алгоритму:

### $$ x_{k+1} = x_k - \frac{\gamma}{\sqrt{\hat{v}_k + \epsilon}} \hat{m}_k $$

---
